{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Run-2 LIQUID NEURAL NETWORK ðŸš€"
      ],
      "metadata": {
        "id": "A5yldhjhWoJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "P7yBVxB3Aeq8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "4R8tNPk8xZmk"
      },
      "outputs": [],
      "source": [
        "# Define the Liquid Neural Network class\n",
        "class LiquidNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, task='cls'):\n",
        "        super(LiquidNeuralNetwork, self).__init__()\n",
        "        self.task = task\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Define the liquid time constant (LTC) layer\n",
        "        self.ltc = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Define the output layer\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through the LTC layer\n",
        "        _, h_n = self.ltc(x)\n",
        "\n",
        "        # Use the last hidden state for prediction\n",
        "        out = self.output_layer(h_n.squeeze(0))\n",
        "\n",
        "        # Apply softmax for classification tasks\n",
        "        if self.task == 'cls':\n",
        "            out = torch.softmax(out, dim=1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the Iris dataset for classification\n",
        "def load_iris_data():\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Load and preprocess the Boston Housing dataset for regression\n",
        "def load_boston_data():\n",
        "    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "    X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "    y = raw_df.values[1::2, 2]\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Train the model\n",
        "def train_model(model, X_train, y_train, X_val, y_val, task='cls', epochs=100, lr=0.01):\n",
        "    criterion = nn.CrossEntropyLoss() if task == 'cls' else nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Training loop with checkpointing\n",
        "    best_val_loss = np.inf\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X.unsqueeze(1))  # Add sequence dimension for RNN\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                outputs = model(batch_X.unsqueeze(1))  # Add sequence dimension for RNN\n",
        "                val_loss += criterion(outputs, batch_y).item()\n",
        "        val_loss /= len(val_loader)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), f'best_model_{task}.pth')\n",
        "            print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, X_test, y_test, task='cls'):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    criterion = nn.CrossEntropyLoss() if task == 'cls' else nn.MSELoss()\n",
        "\n",
        "    # Create DataLoader for testing\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X.unsqueeze(1))  # Add sequence dimension for RNN\n",
        "            test_loss += criterion(outputs, batch_y).item()\n",
        "    test_loss /= len(test_loader)\n",
        "\n",
        "    if task == 'cls':\n",
        "        print(f'Test Accuracy: {1 - test_loss:.4f}')\n",
        "    else:\n",
        "        print(f'Test MSE: {test_loss:.4f}')\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "U2p2KqT5AbBm"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification task (Iris dataset)\n",
        "print(\"Training and evaluating on Iris dataset (Classification)...\")\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = load_iris_data()\n",
        "model_cls = LiquidNeuralNetwork(input_size=4, hidden_size=10, output_size=3, task='cls')\n",
        "train_model(model_cls, X_train, y_train, X_val, y_val, task='cls', epochs=200)\n",
        "evaluate_model(model_cls, X_test, y_test, task='cls')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5PqdGpxAUe1",
        "outputId": "1095bc5d-f000-43d0-8564-74250755c183"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and evaluating on Iris dataset (Classification)...\n",
            "Saved new best model with validation loss: 1.0613\n",
            "Epoch [1/200], Validation Loss: 1.0613\n",
            "Saved new best model with validation loss: 1.0217\n",
            "Epoch [2/200], Validation Loss: 1.0217\n",
            "Saved new best model with validation loss: 0.9833\n",
            "Epoch [3/200], Validation Loss: 0.9833\n",
            "Saved new best model with validation loss: 0.9465\n",
            "Epoch [4/200], Validation Loss: 0.9465\n",
            "Saved new best model with validation loss: 0.9142\n",
            "Epoch [5/200], Validation Loss: 0.9142\n",
            "Saved new best model with validation loss: 0.8881\n",
            "Epoch [6/200], Validation Loss: 0.8881\n",
            "Saved new best model with validation loss: 0.8688\n",
            "Epoch [7/200], Validation Loss: 0.8688\n",
            "Saved new best model with validation loss: 0.8525\n",
            "Epoch [8/200], Validation Loss: 0.8525\n",
            "Saved new best model with validation loss: 0.8394\n",
            "Epoch [9/200], Validation Loss: 0.8394\n",
            "Saved new best model with validation loss: 0.8228\n",
            "Epoch [10/200], Validation Loss: 0.8228\n",
            "Saved new best model with validation loss: 0.8049\n",
            "Epoch [11/200], Validation Loss: 0.8049\n",
            "Saved new best model with validation loss: 0.7855\n",
            "Epoch [12/200], Validation Loss: 0.7855\n",
            "Saved new best model with validation loss: 0.7668\n",
            "Epoch [13/200], Validation Loss: 0.7668\n",
            "Saved new best model with validation loss: 0.7490\n",
            "Epoch [14/200], Validation Loss: 0.7490\n",
            "Saved new best model with validation loss: 0.7328\n",
            "Epoch [15/200], Validation Loss: 0.7328\n",
            "Saved new best model with validation loss: 0.7222\n",
            "Epoch [16/200], Validation Loss: 0.7222\n",
            "Saved new best model with validation loss: 0.7129\n",
            "Epoch [17/200], Validation Loss: 0.7129\n",
            "Saved new best model with validation loss: 0.7044\n",
            "Epoch [18/200], Validation Loss: 0.7044\n",
            "Saved new best model with validation loss: 0.6963\n",
            "Epoch [19/200], Validation Loss: 0.6963\n",
            "Saved new best model with validation loss: 0.6879\n",
            "Epoch [20/200], Validation Loss: 0.6879\n",
            "Saved new best model with validation loss: 0.6794\n",
            "Epoch [21/200], Validation Loss: 0.6794\n",
            "Saved new best model with validation loss: 0.6716\n",
            "Epoch [22/200], Validation Loss: 0.6716\n",
            "Saved new best model with validation loss: 0.6662\n",
            "Epoch [23/200], Validation Loss: 0.6662\n",
            "Saved new best model with validation loss: 0.6623\n",
            "Epoch [24/200], Validation Loss: 0.6623\n",
            "Saved new best model with validation loss: 0.6569\n",
            "Epoch [25/200], Validation Loss: 0.6569\n",
            "Saved new best model with validation loss: 0.6525\n",
            "Epoch [26/200], Validation Loss: 0.6525\n",
            "Saved new best model with validation loss: 0.6489\n",
            "Epoch [27/200], Validation Loss: 0.6489\n",
            "Saved new best model with validation loss: 0.6487\n",
            "Epoch [28/200], Validation Loss: 0.6487\n",
            "Saved new best model with validation loss: 0.6487\n",
            "Epoch [29/200], Validation Loss: 0.6487\n",
            "Epoch [30/200], Validation Loss: 0.6510\n",
            "Epoch [31/200], Validation Loss: 0.6515\n",
            "Epoch [32/200], Validation Loss: 0.6493\n",
            "Epoch [33/200], Validation Loss: 0.6499\n",
            "Saved new best model with validation loss: 0.6458\n",
            "Epoch [34/200], Validation Loss: 0.6458\n",
            "Saved new best model with validation loss: 0.6419\n",
            "Epoch [35/200], Validation Loss: 0.6419\n",
            "Saved new best model with validation loss: 0.6375\n",
            "Epoch [36/200], Validation Loss: 0.6375\n",
            "Epoch [37/200], Validation Loss: 0.6378\n",
            "Saved new best model with validation loss: 0.6372\n",
            "Epoch [38/200], Validation Loss: 0.6372\n",
            "Epoch [39/200], Validation Loss: 0.6401\n",
            "Epoch [40/200], Validation Loss: 0.6399\n",
            "Saved new best model with validation loss: 0.6369\n",
            "Epoch [41/200], Validation Loss: 0.6369\n",
            "Epoch [42/200], Validation Loss: 0.6373\n",
            "Saved new best model with validation loss: 0.6365\n",
            "Epoch [43/200], Validation Loss: 0.6365\n",
            "Saved new best model with validation loss: 0.6341\n",
            "Epoch [44/200], Validation Loss: 0.6341\n",
            "Saved new best model with validation loss: 0.6306\n",
            "Epoch [45/200], Validation Loss: 0.6306\n",
            "Epoch [46/200], Validation Loss: 0.6313\n",
            "Epoch [47/200], Validation Loss: 0.6306\n",
            "Saved new best model with validation loss: 0.6305\n",
            "Epoch [48/200], Validation Loss: 0.6305\n",
            "Epoch [49/200], Validation Loss: 0.6308\n",
            "Saved new best model with validation loss: 0.6287\n",
            "Epoch [50/200], Validation Loss: 0.6287\n",
            "Epoch [51/200], Validation Loss: 0.6299\n",
            "Epoch [52/200], Validation Loss: 0.6298\n",
            "Epoch [53/200], Validation Loss: 0.6290\n",
            "Epoch [54/200], Validation Loss: 0.6308\n",
            "Epoch [55/200], Validation Loss: 0.6317\n",
            "Epoch [56/200], Validation Loss: 0.6321\n",
            "Epoch [57/200], Validation Loss: 0.6318\n",
            "Epoch [58/200], Validation Loss: 0.6297\n",
            "Saved new best model with validation loss: 0.6276\n",
            "Epoch [59/200], Validation Loss: 0.6276\n",
            "Saved new best model with validation loss: 0.6274\n",
            "Epoch [60/200], Validation Loss: 0.6274\n",
            "Epoch [61/200], Validation Loss: 0.6296\n",
            "Epoch [62/200], Validation Loss: 0.6294\n",
            "Epoch [63/200], Validation Loss: 0.6307\n",
            "Epoch [64/200], Validation Loss: 0.6313\n",
            "Epoch [65/200], Validation Loss: 0.6310\n",
            "Epoch [66/200], Validation Loss: 0.6311\n",
            "Epoch [67/200], Validation Loss: 0.6307\n",
            "Epoch [68/200], Validation Loss: 0.6306\n",
            "Epoch [69/200], Validation Loss: 0.6299\n",
            "Epoch [70/200], Validation Loss: 0.6278\n",
            "Epoch [71/200], Validation Loss: 0.6288\n",
            "Epoch [72/200], Validation Loss: 0.6296\n",
            "Epoch [73/200], Validation Loss: 0.6310\n",
            "Epoch [74/200], Validation Loss: 0.6314\n",
            "Epoch [75/200], Validation Loss: 0.6311\n",
            "Epoch [76/200], Validation Loss: 0.6308\n",
            "Epoch [77/200], Validation Loss: 0.6314\n",
            "Epoch [78/200], Validation Loss: 0.6315\n",
            "Epoch [79/200], Validation Loss: 0.6304\n",
            "Epoch [80/200], Validation Loss: 0.6302\n",
            "Epoch [81/200], Validation Loss: 0.6308\n",
            "Epoch [82/200], Validation Loss: 0.6304\n",
            "Epoch [83/200], Validation Loss: 0.6316\n",
            "Epoch [84/200], Validation Loss: 0.6316\n",
            "Epoch [85/200], Validation Loss: 0.6326\n",
            "Epoch [86/200], Validation Loss: 0.6314\n",
            "Epoch [87/200], Validation Loss: 0.6310\n",
            "Epoch [88/200], Validation Loss: 0.6318\n",
            "Epoch [89/200], Validation Loss: 0.6321\n",
            "Epoch [90/200], Validation Loss: 0.6316\n",
            "Epoch [91/200], Validation Loss: 0.6319\n",
            "Epoch [92/200], Validation Loss: 0.6314\n",
            "Epoch [93/200], Validation Loss: 0.6325\n",
            "Epoch [94/200], Validation Loss: 0.6337\n",
            "Epoch [95/200], Validation Loss: 0.6323\n",
            "Epoch [96/200], Validation Loss: 0.6327\n",
            "Epoch [97/200], Validation Loss: 0.6323\n",
            "Epoch [98/200], Validation Loss: 0.6316\n",
            "Epoch [99/200], Validation Loss: 0.6314\n",
            "Epoch [100/200], Validation Loss: 0.6323\n",
            "Epoch [101/200], Validation Loss: 0.6326\n",
            "Epoch [102/200], Validation Loss: 0.6340\n",
            "Epoch [103/200], Validation Loss: 0.6353\n",
            "Epoch [104/200], Validation Loss: 0.6352\n",
            "Epoch [105/200], Validation Loss: 0.6333\n",
            "Epoch [106/200], Validation Loss: 0.6321\n",
            "Epoch [107/200], Validation Loss: 0.6317\n",
            "Epoch [108/200], Validation Loss: 0.6324\n",
            "Epoch [109/200], Validation Loss: 0.6317\n",
            "Epoch [110/200], Validation Loss: 0.6332\n",
            "Epoch [111/200], Validation Loss: 0.6343\n",
            "Epoch [112/200], Validation Loss: 0.6343\n",
            "Epoch [113/200], Validation Loss: 0.6339\n",
            "Epoch [114/200], Validation Loss: 0.6345\n",
            "Epoch [115/200], Validation Loss: 0.6338\n",
            "Epoch [116/200], Validation Loss: 0.6330\n",
            "Epoch [117/200], Validation Loss: 0.6333\n",
            "Epoch [118/200], Validation Loss: 0.6342\n",
            "Epoch [119/200], Validation Loss: 0.6349\n",
            "Epoch [120/200], Validation Loss: 0.6345\n",
            "Epoch [121/200], Validation Loss: 0.6348\n",
            "Epoch [122/200], Validation Loss: 0.6347\n",
            "Epoch [123/200], Validation Loss: 0.6341\n",
            "Epoch [124/200], Validation Loss: 0.6340\n",
            "Epoch [125/200], Validation Loss: 0.6342\n",
            "Epoch [126/200], Validation Loss: 0.6343\n",
            "Epoch [127/200], Validation Loss: 0.6346\n",
            "Epoch [128/200], Validation Loss: 0.6337\n",
            "Epoch [129/200], Validation Loss: 0.6344\n",
            "Epoch [130/200], Validation Loss: 0.6349\n",
            "Epoch [131/200], Validation Loss: 0.6355\n",
            "Epoch [132/200], Validation Loss: 0.6356\n",
            "Epoch [133/200], Validation Loss: 0.6353\n",
            "Epoch [134/200], Validation Loss: 0.6345\n",
            "Epoch [135/200], Validation Loss: 0.6345\n",
            "Epoch [136/200], Validation Loss: 0.6344\n",
            "Epoch [137/200], Validation Loss: 0.6346\n",
            "Epoch [138/200], Validation Loss: 0.6346\n",
            "Epoch [139/200], Validation Loss: 0.6355\n",
            "Epoch [140/200], Validation Loss: 0.6358\n",
            "Epoch [141/200], Validation Loss: 0.6359\n",
            "Epoch [142/200], Validation Loss: 0.6362\n",
            "Epoch [143/200], Validation Loss: 0.6352\n",
            "Epoch [144/200], Validation Loss: 0.6348\n",
            "Epoch [145/200], Validation Loss: 0.6346\n",
            "Epoch [146/200], Validation Loss: 0.6346\n",
            "Epoch [147/200], Validation Loss: 0.6349\n",
            "Epoch [148/200], Validation Loss: 0.6362\n",
            "Epoch [149/200], Validation Loss: 0.6359\n",
            "Epoch [150/200], Validation Loss: 0.6361\n",
            "Epoch [151/200], Validation Loss: 0.6365\n",
            "Epoch [152/200], Validation Loss: 0.6357\n",
            "Epoch [153/200], Validation Loss: 0.6360\n",
            "Epoch [154/200], Validation Loss: 0.6353\n",
            "Epoch [155/200], Validation Loss: 0.6345\n",
            "Epoch [156/200], Validation Loss: 0.6348\n",
            "Epoch [157/200], Validation Loss: 0.6360\n",
            "Epoch [158/200], Validation Loss: 0.6361\n",
            "Epoch [159/200], Validation Loss: 0.6363\n",
            "Epoch [160/200], Validation Loss: 0.6361\n",
            "Epoch [161/200], Validation Loss: 0.6353\n",
            "Epoch [162/200], Validation Loss: 0.6358\n",
            "Epoch [163/200], Validation Loss: 0.6360\n",
            "Epoch [164/200], Validation Loss: 0.6358\n",
            "Epoch [165/200], Validation Loss: 0.6358\n",
            "Epoch [166/200], Validation Loss: 0.6359\n",
            "Epoch [167/200], Validation Loss: 0.6361\n",
            "Epoch [168/200], Validation Loss: 0.6360\n",
            "Epoch [169/200], Validation Loss: 0.6358\n",
            "Epoch [170/200], Validation Loss: 0.6362\n",
            "Epoch [171/200], Validation Loss: 0.6363\n",
            "Epoch [172/200], Validation Loss: 0.6361\n",
            "Epoch [173/200], Validation Loss: 0.6358\n",
            "Epoch [174/200], Validation Loss: 0.6357\n",
            "Epoch [175/200], Validation Loss: 0.6356\n",
            "Epoch [176/200], Validation Loss: 0.6362\n",
            "Epoch [177/200], Validation Loss: 0.6369\n",
            "Epoch [178/200], Validation Loss: 0.6364\n",
            "Epoch [179/200], Validation Loss: 0.6364\n",
            "Epoch [180/200], Validation Loss: 0.6364\n",
            "Epoch [181/200], Validation Loss: 0.6359\n",
            "Epoch [182/200], Validation Loss: 0.6354\n",
            "Epoch [183/200], Validation Loss: 0.6357\n",
            "Epoch [184/200], Validation Loss: 0.6358\n",
            "Epoch [185/200], Validation Loss: 0.6364\n",
            "Epoch [186/200], Validation Loss: 0.6367\n",
            "Epoch [187/200], Validation Loss: 0.6370\n",
            "Epoch [188/200], Validation Loss: 0.6366\n",
            "Epoch [189/200], Validation Loss: 0.6361\n",
            "Epoch [190/200], Validation Loss: 0.6354\n",
            "Epoch [191/200], Validation Loss: 0.6357\n",
            "Epoch [192/200], Validation Loss: 0.6364\n",
            "Epoch [193/200], Validation Loss: 0.6369\n",
            "Epoch [194/200], Validation Loss: 0.6369\n",
            "Epoch [195/200], Validation Loss: 0.6365\n",
            "Epoch [196/200], Validation Loss: 0.6359\n",
            "Epoch [197/200], Validation Loss: 0.6357\n",
            "Epoch [198/200], Validation Loss: 0.6360\n",
            "Epoch [199/200], Validation Loss: 0.6364\n",
            "Epoch [200/200], Validation Loss: 0.6366\n",
            "Test Accuracy: 0.3984\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6016202569007874"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression task (Boston Housing dataset)\n",
        "print(\"\\nTraining and evaluating on Boston Housing dataset (Regression)...\")\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = load_boston_data()\n",
        "model_reg = LiquidNeuralNetwork(input_size=13, hidden_size=10, output_size=1, task='reg')\n",
        "train_model(model_reg, X_train, y_train, X_val, y_val, task='reg', epochs=200)\n",
        "evaluate_model(model_reg, X_test, y_test, task='reg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "287ZNW1UAN8M",
        "outputId": "6fdddc6d-41ad-4e00-9e0d-bfce3b4bef5c"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training and evaluating on Boston Housing dataset (Regression)...\n",
            "Saved new best model with validation loss: 544.9758\n",
            "Epoch [1/200], Validation Loss: 544.9758\n",
            "Saved new best model with validation loss: 503.3520\n",
            "Epoch [2/200], Validation Loss: 503.3520\n",
            "Saved new best model with validation loss: 452.6255\n",
            "Epoch [3/200], Validation Loss: 452.6255\n",
            "Saved new best model with validation loss: 393.3667\n",
            "Epoch [4/200], Validation Loss: 393.3667\n",
            "Saved new best model with validation loss: 333.2827\n",
            "Epoch [5/200], Validation Loss: 333.2827\n",
            "Saved new best model with validation loss: 279.2735\n",
            "Epoch [6/200], Validation Loss: 279.2735\n",
            "Saved new best model with validation loss: 235.1700\n",
            "Epoch [7/200], Validation Loss: 235.1700\n",
            "Saved new best model with validation loss: 200.2098\n",
            "Epoch [8/200], Validation Loss: 200.2098\n",
            "Saved new best model with validation loss: 171.0119\n",
            "Epoch [9/200], Validation Loss: 171.0119\n",
            "Saved new best model with validation loss: 147.3348\n",
            "Epoch [10/200], Validation Loss: 147.3348\n",
            "Saved new best model with validation loss: 128.4387\n",
            "Epoch [11/200], Validation Loss: 128.4387\n",
            "Saved new best model with validation loss: 111.9775\n",
            "Epoch [12/200], Validation Loss: 111.9775\n",
            "Saved new best model with validation loss: 99.4627\n",
            "Epoch [13/200], Validation Loss: 99.4627\n",
            "Saved new best model with validation loss: 89.1915\n",
            "Epoch [14/200], Validation Loss: 89.1915\n",
            "Saved new best model with validation loss: 80.5584\n",
            "Epoch [15/200], Validation Loss: 80.5584\n",
            "Saved new best model with validation loss: 73.4810\n",
            "Epoch [16/200], Validation Loss: 73.4810\n",
            "Saved new best model with validation loss: 67.3092\n",
            "Epoch [17/200], Validation Loss: 67.3092\n",
            "Saved new best model with validation loss: 62.2553\n",
            "Epoch [18/200], Validation Loss: 62.2553\n",
            "Saved new best model with validation loss: 57.9921\n",
            "Epoch [19/200], Validation Loss: 57.9921\n",
            "Saved new best model with validation loss: 54.1804\n",
            "Epoch [20/200], Validation Loss: 54.1804\n",
            "Saved new best model with validation loss: 51.1829\n",
            "Epoch [21/200], Validation Loss: 51.1829\n",
            "Saved new best model with validation loss: 48.7935\n",
            "Epoch [22/200], Validation Loss: 48.7935\n",
            "Saved new best model with validation loss: 46.3148\n",
            "Epoch [23/200], Validation Loss: 46.3148\n",
            "Saved new best model with validation loss: 44.4007\n",
            "Epoch [24/200], Validation Loss: 44.4007\n",
            "Saved new best model with validation loss: 42.7503\n",
            "Epoch [25/200], Validation Loss: 42.7503\n",
            "Saved new best model with validation loss: 41.5409\n",
            "Epoch [26/200], Validation Loss: 41.5409\n",
            "Saved new best model with validation loss: 40.1201\n",
            "Epoch [27/200], Validation Loss: 40.1201\n",
            "Saved new best model with validation loss: 39.1803\n",
            "Epoch [28/200], Validation Loss: 39.1803\n",
            "Epoch [29/200], Validation Loss: 39.5060\n",
            "Saved new best model with validation loss: 37.8670\n",
            "Epoch [30/200], Validation Loss: 37.8670\n",
            "Saved new best model with validation loss: 36.5079\n",
            "Epoch [31/200], Validation Loss: 36.5079\n",
            "Saved new best model with validation loss: 35.0041\n",
            "Epoch [32/200], Validation Loss: 35.0041\n",
            "Saved new best model with validation loss: 33.7727\n",
            "Epoch [33/200], Validation Loss: 33.7727\n",
            "Saved new best model with validation loss: 32.5152\n",
            "Epoch [34/200], Validation Loss: 32.5152\n",
            "Saved new best model with validation loss: 31.2258\n",
            "Epoch [35/200], Validation Loss: 31.2258\n",
            "Saved new best model with validation loss: 30.0822\n",
            "Epoch [36/200], Validation Loss: 30.0822\n",
            "Saved new best model with validation loss: 29.1228\n",
            "Epoch [37/200], Validation Loss: 29.1228\n",
            "Saved new best model with validation loss: 27.8574\n",
            "Epoch [38/200], Validation Loss: 27.8574\n",
            "Saved new best model with validation loss: 26.9048\n",
            "Epoch [39/200], Validation Loss: 26.9048\n",
            "Saved new best model with validation loss: 26.1728\n",
            "Epoch [40/200], Validation Loss: 26.1728\n",
            "Saved new best model with validation loss: 25.4235\n",
            "Epoch [41/200], Validation Loss: 25.4235\n",
            "Saved new best model with validation loss: 24.5357\n",
            "Epoch [42/200], Validation Loss: 24.5357\n",
            "Saved new best model with validation loss: 23.6357\n",
            "Epoch [43/200], Validation Loss: 23.6357\n",
            "Saved new best model with validation loss: 23.4230\n",
            "Epoch [44/200], Validation Loss: 23.4230\n",
            "Saved new best model with validation loss: 22.5209\n",
            "Epoch [45/200], Validation Loss: 22.5209\n",
            "Saved new best model with validation loss: 22.0867\n",
            "Epoch [46/200], Validation Loss: 22.0867\n",
            "Saved new best model with validation loss: 21.9947\n",
            "Epoch [47/200], Validation Loss: 21.9947\n",
            "Saved new best model with validation loss: 21.2938\n",
            "Epoch [48/200], Validation Loss: 21.2938\n",
            "Saved new best model with validation loss: 21.0825\n",
            "Epoch [49/200], Validation Loss: 21.0825\n",
            "Saved new best model with validation loss: 20.8175\n",
            "Epoch [50/200], Validation Loss: 20.8175\n",
            "Saved new best model with validation loss: 20.6180\n",
            "Epoch [51/200], Validation Loss: 20.6180\n",
            "Saved new best model with validation loss: 20.3243\n",
            "Epoch [52/200], Validation Loss: 20.3243\n",
            "Epoch [53/200], Validation Loss: 20.5923\n",
            "Epoch [54/200], Validation Loss: 20.4309\n",
            "Epoch [55/200], Validation Loss: 20.4997\n",
            "Saved new best model with validation loss: 19.8826\n",
            "Epoch [56/200], Validation Loss: 19.8826\n",
            "Saved new best model with validation loss: 19.2535\n",
            "Epoch [57/200], Validation Loss: 19.2535\n",
            "Saved new best model with validation loss: 19.1133\n",
            "Epoch [58/200], Validation Loss: 19.1133\n",
            "Epoch [59/200], Validation Loss: 19.9927\n",
            "Epoch [60/200], Validation Loss: 19.6905\n",
            "Epoch [61/200], Validation Loss: 19.7082\n",
            "Epoch [62/200], Validation Loss: 19.2840\n",
            "Epoch [63/200], Validation Loss: 19.8681\n",
            "Epoch [64/200], Validation Loss: 19.4478\n",
            "Epoch [65/200], Validation Loss: 19.5019\n",
            "Epoch [66/200], Validation Loss: 20.1640\n",
            "Epoch [67/200], Validation Loss: 19.6426\n",
            "Epoch [68/200], Validation Loss: 20.1132\n",
            "Epoch [69/200], Validation Loss: 20.1225\n",
            "Epoch [70/200], Validation Loss: 19.8831\n",
            "Epoch [71/200], Validation Loss: 20.0588\n",
            "Epoch [72/200], Validation Loss: 19.7369\n",
            "Epoch [73/200], Validation Loss: 19.5697\n",
            "Epoch [74/200], Validation Loss: 20.1041\n",
            "Epoch [75/200], Validation Loss: 19.6850\n",
            "Epoch [76/200], Validation Loss: 19.9611\n",
            "Epoch [77/200], Validation Loss: 20.3943\n",
            "Epoch [78/200], Validation Loss: 20.4535\n",
            "Epoch [79/200], Validation Loss: 20.9880\n",
            "Epoch [80/200], Validation Loss: 20.8183\n",
            "Epoch [81/200], Validation Loss: 20.4792\n",
            "Epoch [82/200], Validation Loss: 20.7113\n",
            "Epoch [83/200], Validation Loss: 21.0788\n",
            "Epoch [84/200], Validation Loss: 21.5160\n",
            "Epoch [85/200], Validation Loss: 21.4396\n",
            "Epoch [86/200], Validation Loss: 21.3526\n",
            "Epoch [87/200], Validation Loss: 21.7626\n",
            "Epoch [88/200], Validation Loss: 22.4725\n",
            "Epoch [89/200], Validation Loss: 22.4216\n",
            "Epoch [90/200], Validation Loss: 22.6309\n",
            "Epoch [91/200], Validation Loss: 22.7812\n",
            "Epoch [92/200], Validation Loss: 22.8109\n",
            "Epoch [93/200], Validation Loss: 22.6067\n",
            "Epoch [94/200], Validation Loss: 22.7450\n",
            "Epoch [95/200], Validation Loss: 22.6087\n",
            "Epoch [96/200], Validation Loss: 22.1629\n",
            "Epoch [97/200], Validation Loss: 22.0262\n",
            "Epoch [98/200], Validation Loss: 21.9005\n",
            "Epoch [99/200], Validation Loss: 21.7947\n",
            "Epoch [100/200], Validation Loss: 22.1185\n",
            "Epoch [101/200], Validation Loss: 22.0751\n",
            "Epoch [102/200], Validation Loss: 22.1219\n",
            "Epoch [103/200], Validation Loss: 22.1293\n",
            "Epoch [104/200], Validation Loss: 21.9839\n",
            "Epoch [105/200], Validation Loss: 22.2109\n",
            "Epoch [106/200], Validation Loss: 22.2808\n",
            "Epoch [107/200], Validation Loss: 22.6710\n",
            "Epoch [108/200], Validation Loss: 22.4697\n",
            "Epoch [109/200], Validation Loss: 22.6733\n",
            "Epoch [110/200], Validation Loss: 23.0165\n",
            "Epoch [111/200], Validation Loss: 22.5080\n",
            "Epoch [112/200], Validation Loss: 22.5705\n",
            "Epoch [113/200], Validation Loss: 21.9878\n",
            "Epoch [114/200], Validation Loss: 22.3571\n",
            "Epoch [115/200], Validation Loss: 22.6155\n",
            "Epoch [116/200], Validation Loss: 22.5692\n",
            "Epoch [117/200], Validation Loss: 22.7685\n",
            "Epoch [118/200], Validation Loss: 23.2384\n",
            "Epoch [119/200], Validation Loss: 23.0472\n",
            "Epoch [120/200], Validation Loss: 23.1795\n",
            "Epoch [121/200], Validation Loss: 23.6566\n",
            "Epoch [122/200], Validation Loss: 23.3035\n",
            "Epoch [123/200], Validation Loss: 23.4872\n",
            "Epoch [124/200], Validation Loss: 23.6824\n",
            "Epoch [125/200], Validation Loss: 23.8161\n",
            "Epoch [126/200], Validation Loss: 23.7092\n",
            "Epoch [127/200], Validation Loss: 23.7213\n",
            "Epoch [128/200], Validation Loss: 23.6021\n",
            "Epoch [129/200], Validation Loss: 23.8268\n",
            "Epoch [130/200], Validation Loss: 23.6419\n",
            "Epoch [131/200], Validation Loss: 24.0087\n",
            "Epoch [132/200], Validation Loss: 23.8436\n",
            "Epoch [133/200], Validation Loss: 24.0939\n",
            "Epoch [134/200], Validation Loss: 24.4320\n",
            "Epoch [135/200], Validation Loss: 23.9036\n",
            "Epoch [136/200], Validation Loss: 23.6028\n",
            "Epoch [137/200], Validation Loss: 23.8717\n",
            "Epoch [138/200], Validation Loss: 23.6377\n",
            "Epoch [139/200], Validation Loss: 23.5296\n",
            "Epoch [140/200], Validation Loss: 23.3799\n",
            "Epoch [141/200], Validation Loss: 24.4781\n",
            "Epoch [142/200], Validation Loss: 24.7673\n",
            "Epoch [143/200], Validation Loss: 24.5772\n",
            "Epoch [144/200], Validation Loss: 24.5895\n",
            "Epoch [145/200], Validation Loss: 24.1565\n",
            "Epoch [146/200], Validation Loss: 24.3056\n",
            "Epoch [147/200], Validation Loss: 24.1018\n",
            "Epoch [148/200], Validation Loss: 24.2604\n",
            "Epoch [149/200], Validation Loss: 24.1440\n",
            "Epoch [150/200], Validation Loss: 24.3883\n",
            "Epoch [151/200], Validation Loss: 24.0899\n",
            "Epoch [152/200], Validation Loss: 23.8558\n",
            "Epoch [153/200], Validation Loss: 23.3203\n",
            "Epoch [154/200], Validation Loss: 23.5713\n",
            "Epoch [155/200], Validation Loss: 22.9908\n",
            "Epoch [156/200], Validation Loss: 22.9993\n",
            "Epoch [157/200], Validation Loss: 23.1018\n",
            "Epoch [158/200], Validation Loss: 23.3338\n",
            "Epoch [159/200], Validation Loss: 23.4130\n",
            "Epoch [160/200], Validation Loss: 23.6207\n",
            "Epoch [161/200], Validation Loss: 23.3925\n",
            "Epoch [162/200], Validation Loss: 23.0152\n",
            "Epoch [163/200], Validation Loss: 23.2628\n",
            "Epoch [164/200], Validation Loss: 23.7410\n",
            "Epoch [165/200], Validation Loss: 23.5884\n",
            "Epoch [166/200], Validation Loss: 23.5417\n",
            "Epoch [167/200], Validation Loss: 23.3159\n",
            "Epoch [168/200], Validation Loss: 23.1495\n",
            "Epoch [169/200], Validation Loss: 23.2139\n",
            "Epoch [170/200], Validation Loss: 23.4315\n",
            "Epoch [171/200], Validation Loss: 24.5400\n",
            "Epoch [172/200], Validation Loss: 23.5231\n",
            "Epoch [173/200], Validation Loss: 23.6630\n",
            "Epoch [174/200], Validation Loss: 24.3509\n",
            "Epoch [175/200], Validation Loss: 24.2450\n",
            "Epoch [176/200], Validation Loss: 24.8557\n",
            "Epoch [177/200], Validation Loss: 24.8967\n",
            "Epoch [178/200], Validation Loss: 25.2434\n",
            "Epoch [179/200], Validation Loss: 24.9361\n",
            "Epoch [180/200], Validation Loss: 24.0636\n",
            "Epoch [181/200], Validation Loss: 23.6701\n",
            "Epoch [182/200], Validation Loss: 24.6775\n",
            "Epoch [183/200], Validation Loss: 24.3954\n",
            "Epoch [184/200], Validation Loss: 24.3601\n",
            "Epoch [185/200], Validation Loss: 24.3373\n",
            "Epoch [186/200], Validation Loss: 23.9646\n",
            "Epoch [187/200], Validation Loss: 23.7328\n",
            "Epoch [188/200], Validation Loss: 23.6441\n",
            "Epoch [189/200], Validation Loss: 23.6730\n",
            "Epoch [190/200], Validation Loss: 23.8725\n",
            "Epoch [191/200], Validation Loss: 24.3526\n",
            "Epoch [192/200], Validation Loss: 23.9603\n",
            "Epoch [193/200], Validation Loss: 24.4757\n",
            "Epoch [194/200], Validation Loss: 23.8527\n",
            "Epoch [195/200], Validation Loss: 23.7922\n",
            "Epoch [196/200], Validation Loss: 23.6272\n",
            "Epoch [197/200], Validation Loss: 22.6749\n",
            "Epoch [198/200], Validation Loss: 23.1841\n",
            "Epoch [199/200], Validation Loss: 22.7783\n",
            "Epoch [200/200], Validation Loss: 23.4954\n",
            "Test MSE: 36.0847\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36.084742188453674"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "akne8ntoxkPI"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ§  Run-1 LIQUID NEURAL NETWORK ðŸš€\n",
        "\n",
        "## True Liquid Time-Constant (LTC) Cell\n",
        "\n",
        "- Implemented proper continuous-time dynamics with learnable time constants\n",
        "- Added decay factor based on tau parameter that controls information flow\n",
        "\n",
        "---\n",
        "\n",
        "## Enhanced Architecture\n",
        "\n",
        "- Multi-layer capability for deeper networks\n",
        "- Self-attention mechanism for capturing temporal relationships\n",
        "- Skip connections and layer normalization for better gradient flow\n",
        "\n",
        "---\n",
        "\n",
        "## Robust Training Framework\n",
        "\n",
        "- Learning rate scheduling with `ReduceLROnPlateau`\n",
        "- Early stopping to prevent overfitting\n",
        "- Gradient clipping to prevent exploding gradients\n",
        "- AdamW optimizer with weight decay for regularization\n",
        "\n",
        "---\n",
        "\n",
        "## Comprehensive Evaluation\n",
        "\n",
        "- Detailed metrics for both classification and regression\n",
        "- Visualization of results (confusion matrices, prediction plots)\n",
        "- Feature importance analysis for regression tasks\n",
        "\n",
        "---\n",
        "\n",
        "## Hyperparameter Tuning\n",
        "\n",
        "- Simple grid search to find optimal model configuration\n",
        "- Best model checkpointing\n",
        "\n",
        "---\n",
        "\n",
        "## Improved Data Handling\n",
        "\n",
        "- Better error handling for dataset loading\n",
        "- Proper input normalization\n",
        "- Support for both sequence and non-sequence data formats\n",
        "\n",
        "---\n",
        "\n",
        "The code is now much more robust, handles edge cases better, and should provide significantly better performance on both classification and regression tasks."
      ],
      "metadata": {
        "id": "Q2p1pdazItda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchdiffeq"
      ],
      "metadata": {
        "id": "Yg54t8XwBJAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, confusion_matrix\n",
        "import datetime\n",
        "\n",
        "class LiquidTimeConstantCell(nn.Module):\n",
        "    \"\"\"True Liquid Time-Constant (LTC) cell implementation\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, tau_init=1.0):\n",
        "        super(LiquidTimeConstantCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Learnable time constant parameter (tau)\n",
        "        self.tau = nn.Parameter(torch.ones(hidden_size) * tau_init)\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # Hidden state projection (recurrent weights)\n",
        "        self.hidden_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        # Output nonlinearity\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        # Regularization parameters\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # Input projection\n",
        "        input_term = self.input_proj(x)\n",
        "\n",
        "        # Recurrent projection\n",
        "        hidden_term = self.hidden_proj(hidden)\n",
        "\n",
        "        # Apply time constant dynamics: dh/dt = -h/Ï„ + f(W_iÂ·x + W_hÂ·h)\n",
        "        # This is a discretized version of the ODE\n",
        "        decay = torch.exp(-1.0 / self.tau.abs())\n",
        "        new_hidden = decay * hidden + (1 - decay) * self.activation(input_term + hidden_term)\n",
        "\n",
        "        # Apply dropout for regularization\n",
        "        new_hidden = self.dropout(new_hidden)\n",
        "\n",
        "        return new_hidden\n",
        "\n",
        "class ImprovedLiquidNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=2, task='cls', dropout=0.2):\n",
        "        super(ImprovedLiquidNeuralNetwork, self).__init__()\n",
        "        self.task = task\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Input normalization\n",
        "        self.input_norm = nn.BatchNorm1d(input_size)\n",
        "\n",
        "        # Multi-layer LTC cells\n",
        "        self.ltc_cells = nn.ModuleList()\n",
        "        self.ltc_cells.append(LiquidTimeConstantCell(input_size, hidden_size))\n",
        "        for i in range(1, num_layers):\n",
        "            self.ltc_cells.append(LiquidTimeConstantCell(hidden_size, hidden_size))\n",
        "\n",
        "        # Attention mechanism for temporal dynamics\n",
        "        self.attention = nn.MultiheadAttention(hidden_size, num_heads=4, dropout=dropout)\n",
        "\n",
        "        # Output projection with skip connection\n",
        "        self.pre_output = nn.Linear(hidden_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Layer normalization\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, seq_len, input_size]\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Initialize hidden states\n",
        "        hidden_states = [torch.zeros(batch_size, self.hidden_size, device=x.device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Store outputs from all timesteps for attention\n",
        "        seq_outputs = torch.zeros(seq_len, batch_size, self.hidden_size, device=x.device)\n",
        "\n",
        "        # Process each timestep\n",
        "        for t in range(seq_len):\n",
        "            # Normalize input at each timestep\n",
        "            x_t = x[:, t, :]\n",
        "            if seq_len == 1:  # Handle single timestep case\n",
        "                x_t = self.input_norm(x_t)\n",
        "\n",
        "            # Process through LTC layers\n",
        "            layer_input = x_t\n",
        "            for i, ltc_cell in enumerate(self.ltc_cells):\n",
        "                hidden_states[i] = ltc_cell(layer_input, hidden_states[i])\n",
        "                layer_input = hidden_states[i]\n",
        "\n",
        "            # Store the output from the last LTC layer\n",
        "            seq_outputs[t] = hidden_states[-1]\n",
        "\n",
        "        # Apply attention over the sequence outputs\n",
        "        attn_output, _ = self.attention(seq_outputs, seq_outputs, seq_outputs)\n",
        "\n",
        "        # Get the final output (either last timestep or attention-weighted)\n",
        "        final_output = attn_output[-1]\n",
        "\n",
        "        # Apply layer normalization and skip connection\n",
        "        final_output = self.layer_norm(final_output + hidden_states[-1])\n",
        "\n",
        "        # Final projection\n",
        "        pre_output = F.relu(self.pre_output(final_output))\n",
        "        pre_output = self.dropout(pre_output)\n",
        "        out = self.output_layer(pre_output)\n",
        "\n",
        "        # Apply appropriate activation for task\n",
        "        if self.task == 'cls':\n",
        "            out = F.log_softmax(out, dim=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def init_hidden(self, batch_size, device):\n",
        "        \"\"\"Initialize hidden states\"\"\"\n",
        "        return [torch.zeros(batch_size, self.hidden_size, device=device) for _ in range(self.num_layers)]\n",
        "\n",
        "# Load and preprocess the Iris dataset for classification\n",
        "def load_iris_data():\n",
        "    from sklearn.datasets import load_iris\n",
        "\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.long)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "    # Add sequence dimension if not already present\n",
        "    if X_train.dim() == 2:\n",
        "        X_train = X_train.unsqueeze(1)\n",
        "        X_val = X_val.unsqueeze(1)\n",
        "        X_test = X_test.unsqueeze(1)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, iris.feature_names, iris.target_names\n",
        "\n",
        "# Load and preprocess the Boston Housing dataset for regression\n",
        "def load_boston_data():\n",
        "    try:\n",
        "        from sklearn.datasets import load_boston\n",
        "        boston = load_boston()\n",
        "        X = boston.data\n",
        "        y = boston.target\n",
        "        feature_names = boston.feature_names\n",
        "    except:\n",
        "        # Alternative if sklearn's boston dataset is deprecated\n",
        "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "        try:\n",
        "            raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "            X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "            y = raw_df.values[1::2, 2]\n",
        "            feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "                            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "        except:\n",
        "            print(\"Boston dataset couldn't be loaded!\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Standardize the data\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    # Add sequence dimension if not already present\n",
        "    if X_train.dim() == 2:\n",
        "        X_train = X_train.unsqueeze(1)\n",
        "        X_val = X_val.unsqueeze(1)\n",
        "        X_test = X_test.unsqueeze(1)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test, feature_names\n",
        "\n",
        "# Improved training function with learning rate scheduler and early stopping\n",
        "def train_model(model, X_train, y_train, X_val, y_val, task='cls', epochs=300, lr=0.001,\n",
        "                batch_size=32, patience=20, weight_decay=1e-5):\n",
        "    if task == 'cls':\n",
        "        criterion = nn.NLLLoss()  # Using NLLLoss with log_softmax\n",
        "    else:\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.5)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # For tracking training progress\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Early stopping parameters\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    early_stop_counter = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in val_loader:\n",
        "                outputs = model(batch_X)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_loss)\n",
        "        # After scheduler.step(val_loss):\n",
        "        if scheduler.is_better(val_loss, scheduler.best):\n",
        "            print(f\"Learning rate updated to: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Check for early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            early_stop_counter = 0\n",
        "            # Save the best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "                'train_loss': train_loss\n",
        "            }, f'best_model_{task}.pth')\n",
        "            print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
        "        else:\n",
        "            early_stop_counter += 1\n",
        "\n",
        "        if early_stop_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs. Best epoch was {best_epoch+1} with val_loss {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "    # Load the best model\n",
        "    checkpoint = torch.load(f'best_model_{task}.pth', weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'Training and Validation Loss Curves for {task.upper()} task')\n",
        "    plt.legend()\n",
        "    plt.savefig(f'loss_curves_{task}.png')\n",
        "    plt.close()\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n",
        "# Enhanced evaluation function\n",
        "def evaluate_model(model, X_test, y_test, task='cls', feature_names=None, target_names=None):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for testing\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            if task == 'cls':\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.numpy())\n",
        "                all_targets.extend(batch_y.numpy())\n",
        "            else:\n",
        "                all_preds.extend(outputs.squeeze().numpy())\n",
        "                all_targets.extend(batch_y.squeeze().numpy())\n",
        "\n",
        "    # Prepare metrics\n",
        "    if task == 'cls':\n",
        "        # Classification metrics\n",
        "        acc = accuracy_score(all_targets, all_preds)\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "        print(f'Test Accuracy: {acc:.4f}')\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        # Plot confusion matrix\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.colorbar()\n",
        "\n",
        "        if target_names is not None:\n",
        "            classes = target_names\n",
        "        else:\n",
        "            classes = [f'Class {i}' for i in range(len(cm))]\n",
        "\n",
        "        tick_marks = np.arange(len(classes))\n",
        "        plt.xticks(tick_marks, classes, rotation=45)\n",
        "        plt.yticks(tick_marks, classes)\n",
        "\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('confusion_matrix.png')\n",
        "        plt.close()\n",
        "\n",
        "        return acc\n",
        "    else:\n",
        "        # Regression metrics\n",
        "        mse = mean_squared_error(all_targets, all_preds)\n",
        "        r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "        print(f'Test MSE: {mse:.4f}')\n",
        "        print(f'Test RÂ²: {r2:.4f}')\n",
        "\n",
        "        # Feature importance analysis (for regression)\n",
        "        if feature_names is not None:\n",
        "            print(\"\\nAttempting to analyze feature importance...\")\n",
        "            try:\n",
        "                # Simple feature importance analysis based on input gradients\n",
        "                importance = analyze_feature_importance(model, X_test, feature_names)\n",
        "\n",
        "                # Plot feature importance\n",
        "                plt.figure(figsize=(12, 6))\n",
        "                plt.bar(range(len(importance)), importance)\n",
        "                plt.xticks(range(len(importance)), feature_names, rotation=90)\n",
        "                plt.title('Feature Importance')\n",
        "                plt.tight_layout()\n",
        "                plt.savefig('feature_importance.png')\n",
        "                plt.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Could not analyze feature importance: {e}\")\n",
        "\n",
        "        # Plot predictions vs actual\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.scatter(all_targets, all_preds, alpha=0.5)\n",
        "        plt.plot([min(all_targets), max(all_targets)], [min(all_targets), max(all_targets)], 'r--')\n",
        "        plt.xlabel('Actual')\n",
        "        plt.ylabel('Predicted')\n",
        "        plt.title('Actual vs Predicted Values')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('regression_predictions.png')\n",
        "        plt.close()\n",
        "\n",
        "        return mse, r2\n",
        "\n",
        "# Analyze feature importance through gradient-based methods\n",
        "def analyze_feature_importance(model, X_test, feature_names):\n",
        "    \"\"\"Analyze feature importance by measuring input gradients\"\"\"\n",
        "    importance = np.zeros(len(feature_names))\n",
        "    X = X_test.clone().detach().requires_grad_(True)\n",
        "\n",
        "    model.eval()\n",
        "    output = model(X)\n",
        "    output.sum().backward()\n",
        "\n",
        "    # Compute importance as the mean absolute gradient\n",
        "    importance = X.grad.abs().mean(dim=0).squeeze().numpy()\n",
        "\n",
        "    return importance"
      ],
      "metadata": {
        "id": "TwRTjLRExbq7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICATION TASK - IRIS DATASET\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING AND EVALUATING ON IRIS DATASET (CLASSIFICATION)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, feature_names, target_names = load_iris_data()\n",
        "\n",
        "# Set up tracking for hyperparameter tuning\n",
        "best_val_acc = 0\n",
        "best_hidden_size = 0\n",
        "best_num_layers = 0\n",
        "\n",
        "# Hyperparameter search\n",
        "for hidden_size in [16, 32]:\n",
        "    for num_layers in [1, 2]:\n",
        "        print(f\"\\nTrying hidden_size={hidden_size}, num_layers={num_layers}\")\n",
        "\n",
        "        model_cls = ImprovedLiquidNeuralNetwork(\n",
        "            input_size=4,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=3,\n",
        "            num_layers=num_layers,\n",
        "            task='cls',\n",
        "            dropout=0.2\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        model_cls, _, _ = train_model(\n",
        "            model_cls,\n",
        "            X_train,\n",
        "            y_train,\n",
        "            X_val,\n",
        "            y_val,\n",
        "            task='cls',\n",
        "            epochs=200,\n",
        "            lr=0.001,\n",
        "            batch_size=16,\n",
        "            patience=15\n",
        "        )\n",
        "\n",
        "        # Quick validation check\n",
        "        model_cls.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model_cls(X_val)\n",
        "            _, val_preds = torch.max(val_outputs, 1)\n",
        "            val_acc = accuracy_score(y_val.numpy(), val_preds.numpy())\n",
        "\n",
        "        print(f\"Validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_hidden_size = hidden_size\n",
        "            best_num_layers = num_layers\n",
        "\n",
        "# Train the final model with best hyperparameters\n",
        "print(f\"\\nTraining final classification model with hidden_size={best_hidden_size}, num_layers={best_num_layers}\")\n",
        "final_model_cls = ImprovedLiquidNeuralNetwork(\n",
        "    input_size=4,\n",
        "    hidden_size=best_hidden_size,\n",
        "    output_size=3,\n",
        "    num_layers=best_num_layers,\n",
        "    task='cls'\n",
        ")\n",
        "\n",
        "final_model_cls, _, _ = train_model(\n",
        "    final_model_cls,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    task='cls',\n",
        "    epochs=300\n",
        ")\n",
        "\n",
        "# Evaluate the final model\n",
        "test_acc = evaluate_model(final_model_cls, X_test, y_test, task='cls', target_names=target_names)\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# REGRESSION TASK - BOSTON HOUSING DATASET\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING AND EVALUATING ON BOSTON HOUSING DATASET (REGRESSION)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, feature_names = load_boston_data()\n",
        "\n",
        "    # Set up tracking for hyperparameter tuning\n",
        "    best_val_mse = float('inf')\n",
        "    best_hidden_size = 0\n",
        "    best_num_layers = 0\n",
        "\n",
        "    # Hyperparameter search\n",
        "    for hidden_size in [24, 32]:\n",
        "        for num_layers in [1, 2]:\n",
        "            print(f\"\\nTrying hidden_size={hidden_size}, num_layers={num_layers}\")\n",
        "\n",
        "            model_reg = ImprovedLiquidNeuralNetwork(\n",
        "                input_size=13,\n",
        "                hidden_size=hidden_size,\n",
        "                output_size=1,\n",
        "                num_layers=num_layers,\n",
        "                task='reg',\n",
        "                dropout=0.1\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            model_reg, _, _ = train_model(\n",
        "                model_reg,\n",
        "                X_train,\n",
        "                y_train,\n",
        "                X_val,\n",
        "                y_val,\n",
        "                task='reg',\n",
        "                epochs=200,\n",
        "                lr=0.001,\n",
        "                batch_size=16,\n",
        "                patience=15\n",
        "            )\n",
        "\n",
        "            # Quick validation check\n",
        "            model_reg.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model_reg(X_val)\n",
        "                val_mse = mean_squared_error(y_val.numpy(), val_outputs.numpy())\n",
        "\n",
        "            print(f\"Validation MSE: {val_mse:.4f}\")\n",
        "\n",
        "            if val_mse < best_val_mse:\n",
        "                best_val_mse = val_mse\n",
        "                best_hidden_size = hidden_size\n",
        "                best_num_layers = num_layers\n",
        "\n",
        "    # Train the final model with best hyperparameters\n",
        "    print(f\"\\nTraining final regression model with hidden_size={best_hidden_size}, num_layers={best_num_layers}\")\n",
        "    final_model_reg = ImprovedLiquidNeuralNetwork(\n",
        "        input_size=13,\n",
        "        hidden_size=best_hidden_size,\n",
        "        output_size=1,\n",
        "        num_layers=best_num_layers,\n",
        "        task='reg'\n",
        "    )\n",
        "\n",
        "    final_model_reg, _, _ = train_model(\n",
        "        final_model_reg,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_val,\n",
        "        y_val,\n",
        "        task='reg',\n",
        "        epochs=300\n",
        "    )\n",
        "\n",
        "    # Evaluate the final model\n",
        "    test_mse, test_r2 = evaluate_model(\n",
        "        final_model_reg,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        task='reg',\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    print(f\"Final Test MSE: {test_mse:.4f}, RÂ²: {test_r2:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in regression task: {e}\")\n",
        "    print(\"Skipping Boston Housing dataset evaluation\")"
      ],
      "metadata": {
        "id": "aafGgLmlHV9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICATION TASK - IRIS DATASET\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING AND EVALUATING ON IRIS DATASET (CLASSIFICATION)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test, feature_names, target_names = load_iris_data()\n",
        "\n",
        "# Set up tracking for hyperparameter tuning\n",
        "best_val_acc = 0\n",
        "best_hidden_size = 0\n",
        "best_num_layers = 0\n",
        "best_model_state = None\n",
        "\n",
        "# Hyperparameter search\n",
        "for hidden_size in [16, 32]:\n",
        "    for num_layers in [1, 2]:\n",
        "        print(f\"\\nTrying hidden_size={hidden_size}, num_layers={num_layers}\")\n",
        "\n",
        "        model_cls = ImprovedLiquidNeuralNetwork(\n",
        "            input_size=4,\n",
        "            hidden_size=hidden_size,\n",
        "            output_size=3,\n",
        "            num_layers=num_layers,\n",
        "            task='cls',\n",
        "            dropout=0.2\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        model_cls, _, _ = train_model(\n",
        "            model_cls,\n",
        "            X_train,\n",
        "            y_train,\n",
        "            X_val,\n",
        "            y_val,\n",
        "            task='cls',\n",
        "            epochs=200,\n",
        "            lr=0.001,\n",
        "            batch_size=16,\n",
        "            patience=15\n",
        "        )\n",
        "\n",
        "        # Quick validation check\n",
        "        model_cls.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model_cls(X_val)\n",
        "            _, val_preds = torch.max(val_outputs, 1)\n",
        "            val_acc = accuracy_score(y_val.numpy(), val_preds.numpy())\n",
        "\n",
        "        print(f\"Validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_hidden_size = hidden_size\n",
        "            best_num_layers = num_layers\n",
        "            best_model_state = model_cls.state_dict()\n",
        "\n",
        "            # Save the best model from hyperparameter search\n",
        "            torch.save({\n",
        "                'model_state_dict': best_model_state,\n",
        "                'hyperparams': {\n",
        "                    'hidden_size': best_hidden_size,\n",
        "                    'num_layers': best_num_layers,\n",
        "                    'task': 'cls',\n",
        "                    'input_size': 4,\n",
        "                    'output_size': 3\n",
        "                },\n",
        "                'val_accuracy': best_val_acc,\n",
        "                'feature_names': feature_names,\n",
        "                'target_names': target_names\n",
        "            }, 'best_iris_model_from_search.pth')\n",
        "            print(f\"Saved new best model with validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "# Train the final model with best hyperparameters\n",
        "print(f\"\\nTraining final classification model with hidden_size={best_hidden_size}, num_layers={best_num_layers}\")\n",
        "final_model_cls = ImprovedLiquidNeuralNetwork(\n",
        "    input_size=4,\n",
        "    hidden_size=best_hidden_size,\n",
        "    output_size=3,\n",
        "    num_layers=best_num_layers,\n",
        "    task='cls'\n",
        ")\n",
        "\n",
        "# Option 1: Initialize with the best model from hyperparameter search\n",
        "if best_model_state is not None:\n",
        "    final_model_cls.load_state_dict(best_model_state)\n",
        "    print(\"Initialized final model with best weights from hyperparameter search\")\n",
        "\n",
        "# Train the final model further\n",
        "final_model_cls, train_losses, val_losses = train_model(\n",
        "    final_model_cls,\n",
        "    X_train,\n",
        "    y_train,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    task='cls',\n",
        "    epochs=300\n",
        ")\n",
        "\n",
        "# Save the final trained model with comprehensive metadata\n",
        "torch.save({\n",
        "    'model_state_dict': final_model_cls.state_dict(),\n",
        "    'hyperparams': {\n",
        "        'hidden_size': best_hidden_size,\n",
        "        'num_layers': best_num_layers,\n",
        "        'task': 'cls',\n",
        "        'input_size': 4,\n",
        "        'output_size': 3\n",
        "    },\n",
        "    'training_history': {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses\n",
        "    },\n",
        "    'feature_names': feature_names,\n",
        "    'target_names': target_names,\n",
        "    'metadata': {\n",
        "        'description': 'Improved Liquid Neural Network for Iris classification',\n",
        "        'date_created': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    }\n",
        "}, 'final_iris_model.pth')\n",
        "print(\"Final model saved to 'final_iris_model.pth'\")\n",
        "\n",
        "# Evaluate the final model\n",
        "test_acc = evaluate_model(final_model_cls, X_test, y_test, task='cls', target_names=target_names)\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mz6U5nfIRdJ",
        "outputId": "0bc20206-d589-4ba8-835e-ffac6b5f9ea6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TRAINING AND EVALUATING ON IRIS DATASET (CLASSIFICATION)\n",
            "==================================================\n",
            "\n",
            "Trying hidden_size=16, num_layers=1\n",
            "Epoch [1/200], Train Loss: 1.1041, Val Loss: 1.0481\n",
            "Saved new best model with validation loss: 1.0481\n",
            "Epoch [2/200], Train Loss: 1.0574, Val Loss: 1.0114\n",
            "Saved new best model with validation loss: 1.0114\n",
            "Epoch [3/200], Train Loss: 1.0358, Val Loss: 0.9763\n",
            "Saved new best model with validation loss: 0.9763\n",
            "Epoch [4/200], Train Loss: 0.9955, Val Loss: 0.9430\n",
            "Saved new best model with validation loss: 0.9430\n",
            "Epoch [5/200], Train Loss: 0.9682, Val Loss: 0.9141\n",
            "Saved new best model with validation loss: 0.9141\n",
            "Epoch [6/200], Train Loss: 0.9133, Val Loss: 0.8861\n",
            "Saved new best model with validation loss: 0.8861\n",
            "Epoch [7/200], Train Loss: 0.9001, Val Loss: 0.8631\n",
            "Saved new best model with validation loss: 0.8631\n",
            "Epoch [8/200], Train Loss: 0.8705, Val Loss: 0.8359\n",
            "Saved new best model with validation loss: 0.8359\n",
            "Epoch [9/200], Train Loss: 0.8318, Val Loss: 0.8062\n",
            "Saved new best model with validation loss: 0.8062\n",
            "Epoch [10/200], Train Loss: 0.8143, Val Loss: 0.7749\n",
            "Saved new best model with validation loss: 0.7749\n",
            "Epoch [11/200], Train Loss: 0.7953, Val Loss: 0.7470\n",
            "Saved new best model with validation loss: 0.7470\n",
            "Epoch [12/200], Train Loss: 0.7480, Val Loss: 0.7139\n",
            "Saved new best model with validation loss: 0.7139\n",
            "Epoch [13/200], Train Loss: 0.7528, Val Loss: 0.6809\n",
            "Saved new best model with validation loss: 0.6809\n",
            "Epoch [14/200], Train Loss: 0.7096, Val Loss: 0.6520\n",
            "Saved new best model with validation loss: 0.6520\n",
            "Epoch [15/200], Train Loss: 0.6755, Val Loss: 0.6251\n",
            "Saved new best model with validation loss: 0.6251\n",
            "Epoch [16/200], Train Loss: 0.6302, Val Loss: 0.5871\n",
            "Saved new best model with validation loss: 0.5871\n",
            "Epoch [17/200], Train Loss: 0.6208, Val Loss: 0.5453\n",
            "Saved new best model with validation loss: 0.5453\n",
            "Epoch [18/200], Train Loss: 0.6122, Val Loss: 0.5120\n",
            "Saved new best model with validation loss: 0.5120\n",
            "Epoch [19/200], Train Loss: 0.5435, Val Loss: 0.4727\n",
            "Saved new best model with validation loss: 0.4727\n",
            "Epoch [20/200], Train Loss: 0.5711, Val Loss: 0.4443\n",
            "Saved new best model with validation loss: 0.4443\n",
            "Epoch [21/200], Train Loss: 0.5640, Val Loss: 0.4169\n",
            "Saved new best model with validation loss: 0.4169\n",
            "Epoch [22/200], Train Loss: 0.4676, Val Loss: 0.4110\n",
            "Saved new best model with validation loss: 0.4110\n",
            "Epoch [23/200], Train Loss: 0.4834, Val Loss: 0.3968\n",
            "Saved new best model with validation loss: 0.3968\n",
            "Epoch [24/200], Train Loss: 0.4479, Val Loss: 0.3651\n",
            "Saved new best model with validation loss: 0.3651\n",
            "Epoch [25/200], Train Loss: 0.3637, Val Loss: 0.3371\n",
            "Saved new best model with validation loss: 0.3371\n",
            "Epoch [26/200], Train Loss: 0.3768, Val Loss: 0.3067\n",
            "Saved new best model with validation loss: 0.3067\n",
            "Epoch [27/200], Train Loss: 0.3888, Val Loss: 0.2803\n",
            "Saved new best model with validation loss: 0.2803\n",
            "Epoch [28/200], Train Loss: 0.4531, Val Loss: 0.2615\n",
            "Saved new best model with validation loss: 0.2615\n",
            "Epoch [29/200], Train Loss: 0.3109, Val Loss: 0.2396\n",
            "Saved new best model with validation loss: 0.2396\n",
            "Epoch [30/200], Train Loss: 0.3439, Val Loss: 0.2268\n",
            "Saved new best model with validation loss: 0.2268\n",
            "Epoch [31/200], Train Loss: 0.2884, Val Loss: 0.2062\n",
            "Saved new best model with validation loss: 0.2062\n",
            "Epoch [32/200], Train Loss: 0.3520, Val Loss: 0.2067\n",
            "Epoch [33/200], Train Loss: 0.3500, Val Loss: 0.2254\n",
            "Epoch [34/200], Train Loss: 0.2606, Val Loss: 0.2262\n",
            "Epoch [35/200], Train Loss: 0.2838, Val Loss: 0.2119\n",
            "Epoch [36/200], Train Loss: 0.4438, Val Loss: 0.1796\n",
            "Saved new best model with validation loss: 0.1796\n",
            "Epoch [37/200], Train Loss: 0.2356, Val Loss: 0.1715\n",
            "Saved new best model with validation loss: 0.1715\n",
            "Epoch [38/200], Train Loss: 0.3416, Val Loss: 0.1930\n",
            "Epoch [39/200], Train Loss: 0.2086, Val Loss: 0.2054\n",
            "Epoch [40/200], Train Loss: 0.2282, Val Loss: 0.2176\n",
            "Epoch [41/200], Train Loss: 0.2198, Val Loss: 0.2048\n",
            "Epoch [42/200], Train Loss: 0.2355, Val Loss: 0.1719\n",
            "Epoch [43/200], Train Loss: 0.3100, Val Loss: 0.1532\n",
            "Saved new best model with validation loss: 0.1532\n",
            "Epoch [44/200], Train Loss: 0.3454, Val Loss: 0.1454\n",
            "Saved new best model with validation loss: 0.1454\n",
            "Epoch [45/200], Train Loss: 0.2422, Val Loss: 0.1334\n",
            "Saved new best model with validation loss: 0.1334\n",
            "Epoch [46/200], Train Loss: 0.2493, Val Loss: 0.1465\n",
            "Epoch [47/200], Train Loss: 0.2585, Val Loss: 0.1750\n",
            "Epoch [48/200], Train Loss: 0.2160, Val Loss: 0.1731\n",
            "Epoch [49/200], Train Loss: 0.3205, Val Loss: 0.2341\n",
            "Epoch [50/200], Train Loss: 0.2074, Val Loss: 0.2271\n",
            "Epoch [51/200], Train Loss: 0.5553, Val Loss: 0.1848\n",
            "Epoch [52/200], Train Loss: 0.1819, Val Loss: 0.1840\n",
            "Epoch [53/200], Train Loss: 0.2590, Val Loss: 0.1608\n",
            "Epoch [54/200], Train Loss: 0.2183, Val Loss: 0.1440\n",
            "Epoch [55/200], Train Loss: 0.3733, Val Loss: 0.1206\n",
            "Saved new best model with validation loss: 0.1206\n",
            "Epoch [56/200], Train Loss: 0.1715, Val Loss: 0.1220\n",
            "Epoch [57/200], Train Loss: 0.3431, Val Loss: 0.1577\n",
            "Epoch [58/200], Train Loss: 0.3737, Val Loss: 0.1950\n",
            "Epoch [59/200], Train Loss: 0.2122, Val Loss: 0.1612\n",
            "Epoch [60/200], Train Loss: 0.1914, Val Loss: 0.1515\n",
            "Epoch [61/200], Train Loss: 0.2148, Val Loss: 0.1509\n",
            "Epoch [62/200], Train Loss: 0.1942, Val Loss: 0.1450\n",
            "Epoch [63/200], Train Loss: 0.1681, Val Loss: 0.1505\n",
            "Epoch [64/200], Train Loss: 0.3293, Val Loss: 0.1429\n",
            "Epoch [65/200], Train Loss: 0.2832, Val Loss: 0.1703\n",
            "Epoch [66/200], Train Loss: 0.1523, Val Loss: 0.1678\n",
            "Epoch [67/200], Train Loss: 0.1787, Val Loss: 0.1749\n",
            "Epoch [68/200], Train Loss: 0.4856, Val Loss: 0.1893\n",
            "Epoch [69/200], Train Loss: 0.2105, Val Loss: 0.1578\n",
            "Epoch [70/200], Train Loss: 0.2638, Val Loss: 0.1656\n",
            "Early stopping triggered after 70 epochs. Best epoch was 55 with val_loss 0.1206\n",
            "Validation accuracy: 0.9583\n",
            "Saved new best model with validation accuracy: 0.9583\n",
            "\n",
            "Trying hidden_size=16, num_layers=2\n",
            "Epoch [1/200], Train Loss: 1.1666, Val Loss: 1.0255\n",
            "Saved new best model with validation loss: 1.0255\n",
            "Epoch [2/200], Train Loss: 1.0788, Val Loss: 0.9698\n",
            "Saved new best model with validation loss: 0.9698\n",
            "Epoch [3/200], Train Loss: 1.0083, Val Loss: 0.9144\n",
            "Saved new best model with validation loss: 0.9144\n",
            "Epoch [4/200], Train Loss: 0.9464, Val Loss: 0.8616\n",
            "Saved new best model with validation loss: 0.8616\n",
            "Epoch [5/200], Train Loss: 0.9223, Val Loss: 0.8158\n",
            "Saved new best model with validation loss: 0.8158\n",
            "Epoch [6/200], Train Loss: 0.8544, Val Loss: 0.7715\n",
            "Saved new best model with validation loss: 0.7715\n",
            "Epoch [7/200], Train Loss: 0.8289, Val Loss: 0.7265\n",
            "Saved new best model with validation loss: 0.7265\n",
            "Epoch [8/200], Train Loss: 0.7758, Val Loss: 0.6862\n",
            "Saved new best model with validation loss: 0.6862\n",
            "Epoch [9/200], Train Loss: 0.7134, Val Loss: 0.6491\n",
            "Saved new best model with validation loss: 0.6491\n",
            "Epoch [10/200], Train Loss: 0.6964, Val Loss: 0.6125\n",
            "Saved new best model with validation loss: 0.6125\n",
            "Epoch [11/200], Train Loss: 0.6804, Val Loss: 0.5833\n",
            "Saved new best model with validation loss: 0.5833\n",
            "Epoch [12/200], Train Loss: 0.7044, Val Loss: 0.5547\n",
            "Saved new best model with validation loss: 0.5547\n",
            "Epoch [13/200], Train Loss: 0.6607, Val Loss: 0.5326\n",
            "Saved new best model with validation loss: 0.5326\n",
            "Epoch [14/200], Train Loss: 0.6093, Val Loss: 0.5302\n",
            "Saved new best model with validation loss: 0.5302\n",
            "Epoch [15/200], Train Loss: 0.5707, Val Loss: 0.5177\n",
            "Saved new best model with validation loss: 0.5177\n",
            "Epoch [16/200], Train Loss: 0.5495, Val Loss: 0.4944\n",
            "Saved new best model with validation loss: 0.4944\n",
            "Epoch [17/200], Train Loss: 0.5643, Val Loss: 0.4653\n",
            "Saved new best model with validation loss: 0.4653\n",
            "Epoch [18/200], Train Loss: 0.5080, Val Loss: 0.4360\n",
            "Saved new best model with validation loss: 0.4360\n",
            "Epoch [19/200], Train Loss: 0.5255, Val Loss: 0.4137\n",
            "Saved new best model with validation loss: 0.4137\n",
            "Epoch [20/200], Train Loss: 0.4888, Val Loss: 0.4021\n",
            "Saved new best model with validation loss: 0.4021\n",
            "Epoch [21/200], Train Loss: 0.4794, Val Loss: 0.4250\n",
            "Epoch [22/200], Train Loss: 0.4671, Val Loss: 0.4210\n",
            "Epoch [23/200], Train Loss: 0.4712, Val Loss: 0.3891\n",
            "Saved new best model with validation loss: 0.3891\n",
            "Epoch [24/200], Train Loss: 0.4442, Val Loss: 0.3651\n",
            "Saved new best model with validation loss: 0.3651\n",
            "Epoch [25/200], Train Loss: 0.4631, Val Loss: 0.3598\n",
            "Saved new best model with validation loss: 0.3598\n",
            "Epoch [26/200], Train Loss: 0.4541, Val Loss: 0.3559\n",
            "Saved new best model with validation loss: 0.3559\n",
            "Epoch [27/200], Train Loss: 0.4080, Val Loss: 0.3247\n",
            "Saved new best model with validation loss: 0.3247\n",
            "Epoch [28/200], Train Loss: 0.4425, Val Loss: 0.3088\n",
            "Saved new best model with validation loss: 0.3088\n",
            "Epoch [29/200], Train Loss: 0.4133, Val Loss: 0.3091\n",
            "Epoch [30/200], Train Loss: 0.3787, Val Loss: 0.3170\n",
            "Epoch [31/200], Train Loss: 0.3747, Val Loss: 0.3079\n",
            "Saved new best model with validation loss: 0.3079\n",
            "Epoch [32/200], Train Loss: 0.3825, Val Loss: 0.2849\n",
            "Saved new best model with validation loss: 0.2849\n",
            "Epoch [33/200], Train Loss: 0.3578, Val Loss: 0.2615\n",
            "Saved new best model with validation loss: 0.2615\n",
            "Epoch [34/200], Train Loss: 0.3877, Val Loss: 0.2271\n",
            "Saved new best model with validation loss: 0.2271\n",
            "Epoch [35/200], Train Loss: 0.3261, Val Loss: 0.2327\n",
            "Epoch [36/200], Train Loss: 0.3214, Val Loss: 0.2336\n",
            "Epoch [37/200], Train Loss: 0.3398, Val Loss: 0.2548\n",
            "Epoch [38/200], Train Loss: 0.3750, Val Loss: 0.2308\n",
            "Epoch [39/200], Train Loss: 0.4448, Val Loss: 0.2228\n",
            "Saved new best model with validation loss: 0.2228\n",
            "Epoch [40/200], Train Loss: 0.3584, Val Loss: 0.2096\n",
            "Saved new best model with validation loss: 0.2096\n",
            "Epoch [41/200], Train Loss: 0.3159, Val Loss: 0.2068\n",
            "Saved new best model with validation loss: 0.2068\n",
            "Epoch [42/200], Train Loss: 0.3087, Val Loss: 0.2213\n",
            "Epoch [43/200], Train Loss: 0.2749, Val Loss: 0.1962\n",
            "Saved new best model with validation loss: 0.1962\n",
            "Epoch [44/200], Train Loss: 0.3579, Val Loss: 0.1904\n",
            "Saved new best model with validation loss: 0.1904\n",
            "Epoch [45/200], Train Loss: 0.3112, Val Loss: 0.2023\n",
            "Epoch [46/200], Train Loss: 0.2626, Val Loss: 0.1964\n",
            "Epoch [47/200], Train Loss: 0.2723, Val Loss: 0.2027\n",
            "Epoch [48/200], Train Loss: 0.2904, Val Loss: 0.1717\n",
            "Saved new best model with validation loss: 0.1717\n",
            "Epoch [49/200], Train Loss: 0.3027, Val Loss: 0.1673\n",
            "Saved new best model with validation loss: 0.1673\n",
            "Epoch [50/200], Train Loss: 0.2140, Val Loss: 0.1993\n",
            "Epoch [51/200], Train Loss: 0.4267, Val Loss: 0.1827\n",
            "Epoch [52/200], Train Loss: 0.2704, Val Loss: 0.1825\n",
            "Epoch [53/200], Train Loss: 0.2729, Val Loss: 0.1680\n",
            "Epoch [54/200], Train Loss: 0.2277, Val Loss: 0.1666\n",
            "Saved new best model with validation loss: 0.1666\n",
            "Epoch [55/200], Train Loss: 0.2936, Val Loss: 0.1565\n",
            "Saved new best model with validation loss: 0.1565\n",
            "Epoch [56/200], Train Loss: 0.2827, Val Loss: 0.1381\n",
            "Saved new best model with validation loss: 0.1381\n",
            "Epoch [57/200], Train Loss: 0.2389, Val Loss: 0.1850\n",
            "Epoch [58/200], Train Loss: 0.5098, Val Loss: 0.1879\n",
            "Epoch [59/200], Train Loss: 0.1908, Val Loss: 0.1264\n",
            "Saved new best model with validation loss: 0.1264\n",
            "Epoch [60/200], Train Loss: 0.3054, Val Loss: 0.1191\n",
            "Saved new best model with validation loss: 0.1191\n",
            "Epoch [61/200], Train Loss: 0.1969, Val Loss: 0.1482\n",
            "Epoch [62/200], Train Loss: 0.2066, Val Loss: 0.1643\n",
            "Epoch [63/200], Train Loss: 0.2482, Val Loss: 0.1586\n",
            "Epoch [64/200], Train Loss: 0.4548, Val Loss: 0.1663\n",
            "Epoch [65/200], Train Loss: 0.2309, Val Loss: 0.1792\n",
            "Epoch [66/200], Train Loss: 0.3524, Val Loss: 0.1853\n",
            "Epoch [67/200], Train Loss: 0.2157, Val Loss: 0.1457\n",
            "Epoch [68/200], Train Loss: 0.3425, Val Loss: 0.1693\n",
            "Epoch [69/200], Train Loss: 0.1752, Val Loss: 0.1540\n",
            "Epoch [70/200], Train Loss: 0.2203, Val Loss: 0.1842\n",
            "Epoch [71/200], Train Loss: 0.2414, Val Loss: 0.1776\n",
            "Epoch [72/200], Train Loss: 0.3042, Val Loss: 0.1669\n",
            "Epoch [73/200], Train Loss: 0.2164, Val Loss: 0.1625\n",
            "Epoch [74/200], Train Loss: 0.2570, Val Loss: 0.1379\n",
            "Epoch [75/200], Train Loss: 0.2176, Val Loss: 0.1445\n",
            "Early stopping triggered after 75 epochs. Best epoch was 60 with val_loss 0.1191\n",
            "Validation accuracy: 0.9583\n",
            "\n",
            "Trying hidden_size=32, num_layers=1\n",
            "Epoch [1/200], Train Loss: 0.9899, Val Loss: 0.9038\n",
            "Saved new best model with validation loss: 0.9038\n",
            "Epoch [2/200], Train Loss: 0.8863, Val Loss: 0.7520\n",
            "Saved new best model with validation loss: 0.7520\n",
            "Epoch [3/200], Train Loss: 0.7952, Val Loss: 0.6383\n",
            "Saved new best model with validation loss: 0.6383\n",
            "Epoch [4/200], Train Loss: 0.6773, Val Loss: 0.5521\n",
            "Saved new best model with validation loss: 0.5521\n",
            "Epoch [5/200], Train Loss: 0.6001, Val Loss: 0.4806\n",
            "Saved new best model with validation loss: 0.4806\n",
            "Epoch [6/200], Train Loss: 0.5795, Val Loss: 0.4305\n",
            "Saved new best model with validation loss: 0.4305\n",
            "Epoch [7/200], Train Loss: 0.5145, Val Loss: 0.3783\n",
            "Saved new best model with validation loss: 0.3783\n",
            "Epoch [8/200], Train Loss: 0.4128, Val Loss: 0.3373\n",
            "Saved new best model with validation loss: 0.3373\n",
            "Epoch [9/200], Train Loss: 0.4360, Val Loss: 0.3005\n",
            "Saved new best model with validation loss: 0.3005\n",
            "Epoch [10/200], Train Loss: 0.3918, Val Loss: 0.2775\n",
            "Saved new best model with validation loss: 0.2775\n",
            "Epoch [11/200], Train Loss: 0.3476, Val Loss: 0.2604\n",
            "Saved new best model with validation loss: 0.2604\n",
            "Epoch [12/200], Train Loss: 0.4535, Val Loss: 0.2581\n",
            "Saved new best model with validation loss: 0.2581\n",
            "Epoch [13/200], Train Loss: 0.3071, Val Loss: 0.2631\n",
            "Epoch [14/200], Train Loss: 0.4103, Val Loss: 0.2231\n",
            "Saved new best model with validation loss: 0.2231\n",
            "Epoch [15/200], Train Loss: 0.3526, Val Loss: 0.2060\n",
            "Saved new best model with validation loss: 0.2060\n",
            "Epoch [16/200], Train Loss: 0.3889, Val Loss: 0.1965\n",
            "Saved new best model with validation loss: 0.1965\n",
            "Epoch [17/200], Train Loss: 0.2736, Val Loss: 0.2127\n",
            "Epoch [18/200], Train Loss: 0.2939, Val Loss: 0.2577\n",
            "Epoch [19/200], Train Loss: 0.2848, Val Loss: 0.2201\n",
            "Epoch [20/200], Train Loss: 0.3307, Val Loss: 0.1680\n",
            "Saved new best model with validation loss: 0.1680\n",
            "Epoch [21/200], Train Loss: 0.1901, Val Loss: 0.1665\n",
            "Saved new best model with validation loss: 0.1665\n",
            "Epoch [22/200], Train Loss: 0.2434, Val Loss: 0.1561\n",
            "Saved new best model with validation loss: 0.1561\n",
            "Epoch [23/200], Train Loss: 0.1990, Val Loss: 0.1695\n",
            "Epoch [24/200], Train Loss: 0.1492, Val Loss: 0.1597\n",
            "Epoch [25/200], Train Loss: 0.4045, Val Loss: 0.1371\n",
            "Saved new best model with validation loss: 0.1371\n",
            "Epoch [26/200], Train Loss: 0.2028, Val Loss: 0.1281\n",
            "Saved new best model with validation loss: 0.1281\n",
            "Epoch [27/200], Train Loss: 0.2281, Val Loss: 0.1212\n",
            "Saved new best model with validation loss: 0.1212\n",
            "Epoch [28/200], Train Loss: 0.1664, Val Loss: 0.1297\n",
            "Epoch [29/200], Train Loss: 0.2398, Val Loss: 0.1045\n",
            "Saved new best model with validation loss: 0.1045\n",
            "Epoch [30/200], Train Loss: 0.1328, Val Loss: 0.1117\n",
            "Epoch [31/200], Train Loss: 0.2883, Val Loss: 0.1312\n",
            "Epoch [32/200], Train Loss: 0.1422, Val Loss: 0.1423\n",
            "Epoch [33/200], Train Loss: 0.2208, Val Loss: 0.1565\n",
            "Epoch [34/200], Train Loss: 0.3846, Val Loss: 0.1312\n",
            "Epoch [35/200], Train Loss: 0.1880, Val Loss: 0.1213\n",
            "Epoch [36/200], Train Loss: 0.2666, Val Loss: 0.1509\n",
            "Epoch [37/200], Train Loss: 0.2446, Val Loss: 0.1178\n",
            "Epoch [38/200], Train Loss: 0.2679, Val Loss: 0.1395\n",
            "Epoch [39/200], Train Loss: 0.3313, Val Loss: 0.1480\n",
            "Epoch [40/200], Train Loss: 0.3333, Val Loss: 0.1173\n",
            "Epoch [41/200], Train Loss: 0.1491, Val Loss: 0.1152\n",
            "Epoch [42/200], Train Loss: 0.2350, Val Loss: 0.1190\n",
            "Epoch [43/200], Train Loss: 0.2191, Val Loss: 0.1066\n",
            "Epoch [44/200], Train Loss: 0.0958, Val Loss: 0.1097\n",
            "Early stopping triggered after 44 epochs. Best epoch was 29 with val_loss 0.1045\n",
            "Validation accuracy: 0.9583\n",
            "\n",
            "Trying hidden_size=32, num_layers=2\n",
            "Epoch [1/200], Train Loss: 0.9931, Val Loss: 0.8603\n",
            "Saved new best model with validation loss: 0.8603\n",
            "Epoch [2/200], Train Loss: 0.8776, Val Loss: 0.7001\n",
            "Saved new best model with validation loss: 0.7001\n",
            "Epoch [3/200], Train Loss: 0.7716, Val Loss: 0.5998\n",
            "Saved new best model with validation loss: 0.5998\n",
            "Epoch [4/200], Train Loss: 0.6254, Val Loss: 0.5079\n",
            "Saved new best model with validation loss: 0.5079\n",
            "Epoch [5/200], Train Loss: 0.5524, Val Loss: 0.4547\n",
            "Saved new best model with validation loss: 0.4547\n",
            "Epoch [6/200], Train Loss: 0.4776, Val Loss: 0.4025\n",
            "Saved new best model with validation loss: 0.4025\n",
            "Epoch [7/200], Train Loss: 0.4687, Val Loss: 0.3807\n",
            "Saved new best model with validation loss: 0.3807\n",
            "Epoch [8/200], Train Loss: 0.3991, Val Loss: 0.3431\n",
            "Saved new best model with validation loss: 0.3431\n",
            "Epoch [9/200], Train Loss: 0.4199, Val Loss: 0.3199\n",
            "Saved new best model with validation loss: 0.3199\n",
            "Epoch [10/200], Train Loss: 0.4187, Val Loss: 0.3083\n",
            "Saved new best model with validation loss: 0.3083\n",
            "Epoch [11/200], Train Loss: 0.2940, Val Loss: 0.2400\n",
            "Saved new best model with validation loss: 0.2400\n",
            "Epoch [12/200], Train Loss: 0.3858, Val Loss: 0.2021\n",
            "Saved new best model with validation loss: 0.2021\n",
            "Epoch [13/200], Train Loss: 0.3288, Val Loss: 0.2163\n",
            "Epoch [14/200], Train Loss: 0.3324, Val Loss: 0.1985\n",
            "Saved new best model with validation loss: 0.1985\n",
            "Epoch [15/200], Train Loss: 0.2472, Val Loss: 0.2042\n",
            "Epoch [16/200], Train Loss: 0.2569, Val Loss: 0.2082\n",
            "Epoch [17/200], Train Loss: 0.3355, Val Loss: 0.1701\n",
            "Saved new best model with validation loss: 0.1701\n",
            "Epoch [18/200], Train Loss: 0.2563, Val Loss: 0.1622\n",
            "Saved new best model with validation loss: 0.1622\n",
            "Epoch [19/200], Train Loss: 0.3203, Val Loss: 0.1403\n",
            "Saved new best model with validation loss: 0.1403\n",
            "Epoch [20/200], Train Loss: 0.2183, Val Loss: 0.1252\n",
            "Saved new best model with validation loss: 0.1252\n",
            "Epoch [21/200], Train Loss: 0.3700, Val Loss: 0.1174\n",
            "Saved new best model with validation loss: 0.1174\n",
            "Epoch [22/200], Train Loss: 0.3778, Val Loss: 0.1207\n",
            "Epoch [23/200], Train Loss: 0.3381, Val Loss: 0.1317\n",
            "Epoch [24/200], Train Loss: 0.2953, Val Loss: 0.1382\n",
            "Epoch [25/200], Train Loss: 0.2277, Val Loss: 0.2129\n",
            "Epoch [26/200], Train Loss: 0.1729, Val Loss: 0.2455\n",
            "Epoch [27/200], Train Loss: 0.3197, Val Loss: 0.2119\n",
            "Epoch [28/200], Train Loss: 0.2586, Val Loss: 0.1408\n",
            "Epoch [29/200], Train Loss: 0.3524, Val Loss: 0.1046\n",
            "Saved new best model with validation loss: 0.1046\n",
            "Epoch [30/200], Train Loss: 0.2711, Val Loss: 0.0804\n",
            "Saved new best model with validation loss: 0.0804\n",
            "Epoch [31/200], Train Loss: 0.2953, Val Loss: 0.0899\n",
            "Epoch [32/200], Train Loss: 0.1366, Val Loss: 0.1048\n",
            "Epoch [33/200], Train Loss: 0.2859, Val Loss: 0.0902\n",
            "Epoch [34/200], Train Loss: 0.2785, Val Loss: 0.1074\n",
            "Epoch [35/200], Train Loss: 0.3757, Val Loss: 0.1969\n",
            "Epoch [36/200], Train Loss: 0.3591, Val Loss: 0.2218\n",
            "Epoch [37/200], Train Loss: 0.2256, Val Loss: 0.1773\n",
            "Epoch [38/200], Train Loss: 0.2684, Val Loss: 0.1332\n",
            "Epoch [39/200], Train Loss: 0.3346, Val Loss: 0.1099\n",
            "Epoch [40/200], Train Loss: 0.2843, Val Loss: 0.1036\n",
            "Epoch [41/200], Train Loss: 0.2124, Val Loss: 0.1064\n",
            "Epoch [42/200], Train Loss: 0.2409, Val Loss: 0.1299\n",
            "Epoch [43/200], Train Loss: 0.3381, Val Loss: 0.1337\n",
            "Epoch [44/200], Train Loss: 0.1842, Val Loss: 0.1304\n",
            "Epoch [45/200], Train Loss: 0.2247, Val Loss: 0.1487\n",
            "Early stopping triggered after 45 epochs. Best epoch was 30 with val_loss 0.0804\n",
            "Validation accuracy: 1.0000\n",
            "Saved new best model with validation accuracy: 1.0000\n",
            "\n",
            "Training final classification model with hidden_size=32, num_layers=2\n",
            "Initialized final model with best weights from hyperparameter search\n",
            "Epoch [1/300], Train Loss: 0.2568, Val Loss: 0.1320\n",
            "Saved new best model with validation loss: 0.1320\n",
            "Epoch [2/300], Train Loss: 0.1330, Val Loss: 0.1560\n",
            "Epoch [3/300], Train Loss: 0.1894, Val Loss: 0.2228\n",
            "Epoch [4/300], Train Loss: 0.1626, Val Loss: 0.2168\n",
            "Epoch [5/300], Train Loss: 0.1529, Val Loss: 0.1792\n",
            "Epoch [6/300], Train Loss: 0.2073, Val Loss: 0.1477\n",
            "Epoch [7/300], Train Loss: 0.1854, Val Loss: 0.1329\n",
            "Epoch [8/300], Train Loss: 0.1132, Val Loss: 0.1300\n",
            "Saved new best model with validation loss: 0.1300\n",
            "Epoch [9/300], Train Loss: 0.1344, Val Loss: 0.1243\n",
            "Saved new best model with validation loss: 0.1243\n",
            "Epoch [10/300], Train Loss: 0.0864, Val Loss: 0.1323\n",
            "Epoch [11/300], Train Loss: 0.1530, Val Loss: 0.1696\n",
            "Epoch [12/300], Train Loss: 0.1164, Val Loss: 0.1984\n",
            "Epoch [13/300], Train Loss: 0.0826, Val Loss: 0.2524\n",
            "Epoch [14/300], Train Loss: 0.1687, Val Loss: 0.2819\n",
            "Epoch [15/300], Train Loss: 0.1417, Val Loss: 0.2628\n",
            "Epoch [16/300], Train Loss: 0.0576, Val Loss: 0.2247\n",
            "Epoch [17/300], Train Loss: 0.3461, Val Loss: 0.2003\n",
            "Epoch [18/300], Train Loss: 0.0782, Val Loss: 0.1598\n",
            "Epoch [19/300], Train Loss: 0.3594, Val Loss: 0.1474\n",
            "Epoch [20/300], Train Loss: 0.0682, Val Loss: 0.1520\n",
            "Epoch [21/300], Train Loss: 0.1398, Val Loss: 0.1620\n",
            "Epoch [22/300], Train Loss: 0.1225, Val Loss: 0.1753\n",
            "Epoch [23/300], Train Loss: 0.0811, Val Loss: 0.1912\n",
            "Epoch [24/300], Train Loss: 0.1145, Val Loss: 0.1911\n",
            "Epoch [25/300], Train Loss: 0.1110, Val Loss: 0.2011\n",
            "Epoch [26/300], Train Loss: 0.1486, Val Loss: 0.1937\n",
            "Epoch [27/300], Train Loss: 0.3954, Val Loss: 0.1957\n",
            "Epoch [28/300], Train Loss: 0.0987, Val Loss: 0.1953\n",
            "Epoch [29/300], Train Loss: 0.1183, Val Loss: 0.1992\n",
            "Early stopping triggered after 29 epochs. Best epoch was 9 with val_loss 0.1243\n",
            "Final model saved to 'final_iris_model.pth'\n",
            "Test Accuracy: 1.0000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10  0  0]\n",
            " [ 0  9  0]\n",
            " [ 0  0 11]]\n",
            "Final Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# REGRESSION TASK - BOSTON HOUSING DATASET\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"TRAINING AND EVALUATING ON BOSTON HOUSING DATASET (REGRESSION)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "try:\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test, feature_names = load_boston_data()\n",
        "\n",
        "    # Set up tracking for hyperparameter tuning\n",
        "    best_val_mse = float('inf')\n",
        "    best_hidden_size = 0\n",
        "    best_num_layers = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    # Hyperparameter search\n",
        "    for hidden_size in [24, 32]:\n",
        "        for num_layers in [1, 2]:\n",
        "            print(f\"\\nTrying hidden_size={hidden_size}, num_layers={num_layers}\")\n",
        "\n",
        "            model_reg = ImprovedLiquidNeuralNetwork(\n",
        "                input_size=13,\n",
        "                hidden_size=hidden_size,\n",
        "                output_size=1,\n",
        "                num_layers=num_layers,\n",
        "                task='reg',\n",
        "                dropout=0.1\n",
        "            )\n",
        "\n",
        "            # Train the model\n",
        "            model_reg, _, _ = train_model(\n",
        "                model_reg,\n",
        "                X_train,\n",
        "                y_train,\n",
        "                X_val,\n",
        "                y_val,\n",
        "                task='reg',\n",
        "                epochs=200,\n",
        "                lr=0.001,\n",
        "                batch_size=16,\n",
        "                patience=15\n",
        "            )\n",
        "\n",
        "            # Quick validation check\n",
        "            model_reg.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model_reg(X_val)\n",
        "                val_mse = mean_squared_error(y_val.numpy(), val_outputs.numpy())\n",
        "\n",
        "            print(f\"Validation MSE: {val_mse:.4f}\")\n",
        "\n",
        "            if val_mse < best_val_mse:\n",
        "                best_val_mse = val_mse\n",
        "                best_hidden_size = hidden_size\n",
        "                best_num_layers = num_layers\n",
        "                best_model_state = model_reg.state_dict()\n",
        "\n",
        "                # Save the best model from hyperparameter search\n",
        "                torch.save({\n",
        "                    'model_state_dict': best_model_state,\n",
        "                    'hyperparams': {\n",
        "                        'hidden_size': best_hidden_size,\n",
        "                        'num_layers': best_num_layers,\n",
        "                        'task': 'reg',\n",
        "                        'input_size': 13,\n",
        "                        'output_size': 1\n",
        "                    },\n",
        "                    'val_mse': best_val_mse,\n",
        "                    'feature_names': feature_names\n",
        "                }, 'best_boston_model_from_search.pth')\n",
        "                print(f\"Saved new best model with validation MSE: {best_val_mse:.4f}\")\n",
        "\n",
        "    # Train the final model with best hyperparameters\n",
        "    print(f\"\\nTraining final regression model with hidden_size={best_hidden_size}, num_layers={best_num_layers}\")\n",
        "    final_model_reg = ImprovedLiquidNeuralNetwork(\n",
        "        input_size=13,\n",
        "        hidden_size=best_hidden_size,\n",
        "        output_size=1,\n",
        "        num_layers=best_num_layers,\n",
        "        task='reg'\n",
        "    )\n",
        "\n",
        "    # Option 1: Initialize with the best model from hyperparameter search\n",
        "    if best_model_state is not None:\n",
        "        final_model_reg.load_state_dict(best_model_state)\n",
        "        print(\"Initialized final model with best weights from hyperparameter search\")\n",
        "\n",
        "    # Train the final model further\n",
        "    final_model_reg, train_losses, val_losses = train_model(\n",
        "        final_model_reg,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_val,\n",
        "        y_val,\n",
        "        task='reg',\n",
        "        epochs=300\n",
        "    )\n",
        "\n",
        "    # Save the final trained model with comprehensive metadata\n",
        "    torch.save({\n",
        "        'model_state_dict': final_model_reg.state_dict(),\n",
        "        'hyperparams': {\n",
        "            'hidden_size': best_hidden_size,\n",
        "            'num_layers': best_num_layers,\n",
        "            'task': 'reg',\n",
        "            'input_size': 13,\n",
        "            'output_size': 1\n",
        "        },\n",
        "        'training_history': {\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses\n",
        "        },\n",
        "        'feature_names': feature_names,\n",
        "        'metadata': {\n",
        "            'description': 'Improved Liquid Neural Network for Boston housing price regression',\n",
        "            'date_created': datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        }\n",
        "    }, 'final_boston_model.pth')\n",
        "    print(\"Final model saved to 'final_boston_model.pth'\")\n",
        "\n",
        "    # Evaluate the final model\n",
        "    test_mse, test_r2 = evaluate_model(\n",
        "        final_model_reg,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        task='reg',\n",
        "        feature_names=feature_names\n",
        "    )\n",
        "    print(f\"Final Test MSE: {test_mse:.4f}, RÂ²: {test_r2:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in regression task: {e}\")\n",
        "    print(\"Skipping Boston Housing dataset evaluation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDq0amOxOB1p",
        "outputId": "1a4407da-6112-4a29-f396-b4ce234ad26b"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "TRAINING AND EVALUATING ON BOSTON HOUSING DATASET (REGRESSION)\n",
            "==================================================\n",
            "\n",
            "Trying hidden_size=24, num_layers=1\n",
            "Epoch [1/200], Train Loss: 601.7182, Val Loss: 590.8112\n",
            "Saved new best model with validation loss: 590.8112\n",
            "Epoch [2/200], Train Loss: 612.4411, Val Loss: 541.4534\n",
            "Saved new best model with validation loss: 541.4534\n",
            "Epoch [3/200], Train Loss: 500.2608, Val Loss: 464.7749\n",
            "Saved new best model with validation loss: 464.7749\n",
            "Epoch [4/200], Train Loss: 434.9375, Val Loss: 380.8878\n",
            "Saved new best model with validation loss: 380.8878\n",
            "Epoch [5/200], Train Loss: 357.0971, Val Loss: 287.6704\n",
            "Saved new best model with validation loss: 287.6704\n",
            "Epoch [6/200], Train Loss: 253.6115, Val Loss: 194.1476\n",
            "Saved new best model with validation loss: 194.1476\n",
            "Epoch [7/200], Train Loss: 180.3155, Val Loss: 113.7186\n",
            "Saved new best model with validation loss: 113.7186\n",
            "Epoch [8/200], Train Loss: 112.1518, Val Loss: 66.7696\n",
            "Saved new best model with validation loss: 66.7696\n",
            "Epoch [9/200], Train Loss: 84.5962, Val Loss: 46.0674\n",
            "Saved new best model with validation loss: 46.0674\n",
            "Epoch [10/200], Train Loss: 77.1247, Val Loss: 39.1287\n",
            "Saved new best model with validation loss: 39.1287\n",
            "Epoch [11/200], Train Loss: 64.4376, Val Loss: 34.6585\n",
            "Saved new best model with validation loss: 34.6585\n",
            "Epoch [12/200], Train Loss: 61.6078, Val Loss: 29.4160\n",
            "Saved new best model with validation loss: 29.4160\n",
            "Epoch [13/200], Train Loss: 52.6947, Val Loss: 25.7030\n",
            "Saved new best model with validation loss: 25.7030\n",
            "Epoch [14/200], Train Loss: 51.1024, Val Loss: 25.1212\n",
            "Saved new best model with validation loss: 25.1212\n",
            "Epoch [15/200], Train Loss: 44.8752, Val Loss: 22.9478\n",
            "Saved new best model with validation loss: 22.9478\n",
            "Epoch [16/200], Train Loss: 39.0887, Val Loss: 23.1495\n",
            "Epoch [17/200], Train Loss: 39.7657, Val Loss: 22.3422\n",
            "Saved new best model with validation loss: 22.3422\n",
            "Epoch [18/200], Train Loss: 36.8542, Val Loss: 22.5831\n",
            "Epoch [19/200], Train Loss: 36.2600, Val Loss: 20.8769\n",
            "Saved new best model with validation loss: 20.8769\n",
            "Epoch [20/200], Train Loss: 38.4491, Val Loss: 22.8562\n",
            "Epoch [21/200], Train Loss: 37.0011, Val Loss: 21.7770\n",
            "Epoch [22/200], Train Loss: 35.1650, Val Loss: 21.1858\n",
            "Epoch [23/200], Train Loss: 27.3619, Val Loss: 21.1455\n",
            "Epoch [24/200], Train Loss: 34.5196, Val Loss: 25.2609\n",
            "Epoch [25/200], Train Loss: 35.2578, Val Loss: 21.3810\n",
            "Epoch [26/200], Train Loss: 33.1565, Val Loss: 20.2082\n",
            "Saved new best model with validation loss: 20.2082\n",
            "Epoch [27/200], Train Loss: 34.8660, Val Loss: 19.6705\n",
            "Saved new best model with validation loss: 19.6705\n",
            "Epoch [28/200], Train Loss: 35.4636, Val Loss: 20.3717\n",
            "Epoch [29/200], Train Loss: 30.4413, Val Loss: 19.5114\n",
            "Saved new best model with validation loss: 19.5114\n",
            "Epoch [30/200], Train Loss: 33.9650, Val Loss: 22.1831\n",
            "Epoch [31/200], Train Loss: 28.6361, Val Loss: 20.5490\n",
            "Epoch [32/200], Train Loss: 30.7695, Val Loss: 22.6522\n",
            "Epoch [33/200], Train Loss: 27.2110, Val Loss: 21.0115\n",
            "Epoch [34/200], Train Loss: 21.1757, Val Loss: 20.2676\n",
            "Epoch [35/200], Train Loss: 32.3477, Val Loss: 19.5139\n",
            "Epoch [36/200], Train Loss: 26.3441, Val Loss: 19.6522\n",
            "Epoch [37/200], Train Loss: 28.7322, Val Loss: 20.8743\n",
            "Epoch [38/200], Train Loss: 27.5276, Val Loss: 23.9787\n",
            "Epoch [39/200], Train Loss: 27.2185, Val Loss: 21.7648\n",
            "Epoch [40/200], Train Loss: 24.6699, Val Loss: 22.0671\n",
            "Epoch [41/200], Train Loss: 30.6510, Val Loss: 22.0797\n",
            "Epoch [42/200], Train Loss: 28.2881, Val Loss: 21.3526\n",
            "Epoch [43/200], Train Loss: 28.1038, Val Loss: 21.4724\n",
            "Epoch [44/200], Train Loss: 27.0474, Val Loss: 21.2695\n",
            "Early stopping triggered after 44 epochs. Best epoch was 29 with val_loss 19.5114\n",
            "Validation MSE: 20.9685\n",
            "Saved new best model with validation MSE: 20.9685\n",
            "\n",
            "Trying hidden_size=24, num_layers=2\n",
            "Epoch [1/200], Train Loss: 597.3890, Val Loss: 596.6041\n",
            "Saved new best model with validation loss: 596.6041\n",
            "Epoch [2/200], Train Loss: 598.9057, Val Loss: 563.4557\n",
            "Saved new best model with validation loss: 563.4557\n",
            "Epoch [3/200], Train Loss: 542.6733, Val Loss: 525.3709\n",
            "Saved new best model with validation loss: 525.3709\n",
            "Epoch [4/200], Train Loss: 502.8297, Val Loss: 477.9230\n",
            "Saved new best model with validation loss: 477.9230\n",
            "Epoch [5/200], Train Loss: 448.1224, Val Loss: 421.8710\n",
            "Saved new best model with validation loss: 421.8710\n",
            "Epoch [6/200], Train Loss: 399.1444, Val Loss: 359.6456\n",
            "Saved new best model with validation loss: 359.6456\n",
            "Epoch [7/200], Train Loss: 334.1509, Val Loss: 293.8275\n",
            "Saved new best model with validation loss: 293.8275\n",
            "Epoch [8/200], Train Loss: 271.6111, Val Loss: 228.2846\n",
            "Saved new best model with validation loss: 228.2846\n",
            "Epoch [9/200], Train Loss: 216.5298, Val Loss: 166.8853\n",
            "Saved new best model with validation loss: 166.8853\n",
            "Epoch [10/200], Train Loss: 158.6628, Val Loss: 114.6528\n",
            "Saved new best model with validation loss: 114.6528\n",
            "Epoch [11/200], Train Loss: 131.5856, Val Loss: 78.7660\n",
            "Saved new best model with validation loss: 78.7660\n",
            "Epoch [12/200], Train Loss: 102.6673, Val Loss: 63.8006\n",
            "Saved new best model with validation loss: 63.8006\n",
            "Epoch [13/200], Train Loss: 85.0624, Val Loss: 43.6051\n",
            "Saved new best model with validation loss: 43.6051\n",
            "Epoch [14/200], Train Loss: 80.8513, Val Loss: 34.2911\n",
            "Saved new best model with validation loss: 34.2911\n",
            "Epoch [15/200], Train Loss: 60.3374, Val Loss: 29.2124\n",
            "Saved new best model with validation loss: 29.2124\n",
            "Epoch [16/200], Train Loss: 57.1206, Val Loss: 26.0369\n",
            "Saved new best model with validation loss: 26.0369\n",
            "Epoch [17/200], Train Loss: 60.1723, Val Loss: 26.1990\n",
            "Epoch [18/200], Train Loss: 43.5475, Val Loss: 23.3355\n",
            "Saved new best model with validation loss: 23.3355\n",
            "Epoch [19/200], Train Loss: 49.1786, Val Loss: 24.3381\n",
            "Epoch [20/200], Train Loss: 48.7312, Val Loss: 22.1699\n",
            "Saved new best model with validation loss: 22.1699\n",
            "Epoch [21/200], Train Loss: 47.6501, Val Loss: 21.7280\n",
            "Saved new best model with validation loss: 21.7280\n",
            "Epoch [22/200], Train Loss: 34.4220, Val Loss: 20.9196\n",
            "Saved new best model with validation loss: 20.9196\n",
            "Epoch [23/200], Train Loss: 40.8777, Val Loss: 21.4229\n",
            "Epoch [24/200], Train Loss: 40.9034, Val Loss: 23.3750\n",
            "Epoch [25/200], Train Loss: 41.4015, Val Loss: 20.2089\n",
            "Saved new best model with validation loss: 20.2089\n",
            "Epoch [26/200], Train Loss: 40.0832, Val Loss: 23.6579\n",
            "Epoch [27/200], Train Loss: 36.0678, Val Loss: 21.7306\n",
            "Epoch [28/200], Train Loss: 38.4062, Val Loss: 22.5612\n",
            "Epoch [29/200], Train Loss: 35.9915, Val Loss: 25.3620\n",
            "Epoch [30/200], Train Loss: 33.5213, Val Loss: 28.2953\n",
            "Epoch [31/200], Train Loss: 59.9174, Val Loss: 24.9718\n",
            "Epoch [32/200], Train Loss: 33.6352, Val Loss: 21.6760\n",
            "Epoch [33/200], Train Loss: 43.0163, Val Loss: 22.0050\n",
            "Epoch [34/200], Train Loss: 42.8075, Val Loss: 23.2538\n",
            "Epoch [35/200], Train Loss: 39.4291, Val Loss: 22.3826\n",
            "Epoch [36/200], Train Loss: 36.4348, Val Loss: 20.5096\n",
            "Epoch [37/200], Train Loss: 30.2103, Val Loss: 22.7773\n",
            "Epoch [38/200], Train Loss: 29.3271, Val Loss: 22.8699\n",
            "Epoch [39/200], Train Loss: 41.7292, Val Loss: 21.7968\n",
            "Epoch [40/200], Train Loss: 26.7839, Val Loss: 21.1779\n",
            "Early stopping triggered after 40 epochs. Best epoch was 25 with val_loss 20.2089\n",
            "Validation MSE: 21.3370\n",
            "\n",
            "Trying hidden_size=32, num_layers=1\n",
            "Epoch [1/200], Train Loss: 594.6635, Val Loss: 576.8085\n",
            "Saved new best model with validation loss: 576.8085\n",
            "Epoch [2/200], Train Loss: 545.0841, Val Loss: 528.3799\n",
            "Saved new best model with validation loss: 528.3799\n",
            "Epoch [3/200], Train Loss: 521.3955, Val Loss: 469.9994\n",
            "Saved new best model with validation loss: 469.9994\n",
            "Epoch [4/200], Train Loss: 435.7885, Val Loss: 391.6653\n",
            "Saved new best model with validation loss: 391.6653\n",
            "Epoch [5/200], Train Loss: 356.4667, Val Loss: 292.1607\n",
            "Saved new best model with validation loss: 292.1607\n",
            "Epoch [6/200], Train Loss: 262.2109, Val Loss: 198.8292\n",
            "Saved new best model with validation loss: 198.8292\n",
            "Epoch [7/200], Train Loss: 176.6524, Val Loss: 116.9103\n",
            "Saved new best model with validation loss: 116.9103\n",
            "Epoch [8/200], Train Loss: 121.1482, Val Loss: 64.4015\n",
            "Saved new best model with validation loss: 64.4015\n",
            "Epoch [9/200], Train Loss: 86.4240, Val Loss: 45.9455\n",
            "Saved new best model with validation loss: 45.9455\n",
            "Epoch [10/200], Train Loss: 77.6840, Val Loss: 38.1401\n",
            "Saved new best model with validation loss: 38.1401\n",
            "Epoch [11/200], Train Loss: 70.2116, Val Loss: 34.0903\n",
            "Saved new best model with validation loss: 34.0903\n",
            "Epoch [12/200], Train Loss: 62.7709, Val Loss: 27.4110\n",
            "Saved new best model with validation loss: 27.4110\n",
            "Epoch [13/200], Train Loss: 50.4073, Val Loss: 23.3783\n",
            "Saved new best model with validation loss: 23.3783\n",
            "Epoch [14/200], Train Loss: 40.1728, Val Loss: 21.2437\n",
            "Saved new best model with validation loss: 21.2437\n",
            "Epoch [15/200], Train Loss: 40.9124, Val Loss: 20.6746\n",
            "Saved new best model with validation loss: 20.6746\n",
            "Epoch [16/200], Train Loss: 38.5556, Val Loss: 20.4714\n",
            "Saved new best model with validation loss: 20.4714\n",
            "Epoch [17/200], Train Loss: 30.4050, Val Loss: 22.7845\n",
            "Epoch [18/200], Train Loss: 49.1847, Val Loss: 20.6181\n",
            "Epoch [19/200], Train Loss: 31.2567, Val Loss: 20.1955\n",
            "Saved new best model with validation loss: 20.1955\n",
            "Epoch [20/200], Train Loss: 30.3076, Val Loss: 20.9184\n",
            "Epoch [21/200], Train Loss: 26.9916, Val Loss: 19.0653\n",
            "Saved new best model with validation loss: 19.0653\n",
            "Epoch [22/200], Train Loss: 28.7372, Val Loss: 19.2076\n",
            "Epoch [23/200], Train Loss: 23.6429, Val Loss: 20.3010\n",
            "Epoch [24/200], Train Loss: 35.4425, Val Loss: 20.2651\n",
            "Epoch [25/200], Train Loss: 29.7434, Val Loss: 20.3507\n",
            "Epoch [26/200], Train Loss: 33.9818, Val Loss: 18.4570\n",
            "Saved new best model with validation loss: 18.4570\n",
            "Epoch [27/200], Train Loss: 28.1035, Val Loss: 18.9035\n",
            "Epoch [28/200], Train Loss: 24.0982, Val Loss: 18.3987\n",
            "Saved new best model with validation loss: 18.3987\n",
            "Epoch [29/200], Train Loss: 30.8555, Val Loss: 18.7475\n",
            "Epoch [30/200], Train Loss: 24.9507, Val Loss: 18.0434\n",
            "Saved new best model with validation loss: 18.0434\n",
            "Epoch [31/200], Train Loss: 19.8768, Val Loss: 20.8715\n",
            "Epoch [32/200], Train Loss: 31.7979, Val Loss: 22.1772\n",
            "Epoch [33/200], Train Loss: 33.3021, Val Loss: 18.8381\n",
            "Epoch [34/200], Train Loss: 26.7788, Val Loss: 19.4365\n",
            "Epoch [35/200], Train Loss: 24.6094, Val Loss: 19.9372\n",
            "Epoch [36/200], Train Loss: 26.5128, Val Loss: 20.8273\n",
            "Epoch [37/200], Train Loss: 28.5013, Val Loss: 19.0505\n",
            "Epoch [38/200], Train Loss: 31.7897, Val Loss: 18.8154\n",
            "Epoch [39/200], Train Loss: 28.3832, Val Loss: 18.9500\n",
            "Epoch [40/200], Train Loss: 37.5172, Val Loss: 23.5237\n",
            "Epoch [41/200], Train Loss: 28.0406, Val Loss: 21.8649\n",
            "Epoch [42/200], Train Loss: 30.9250, Val Loss: 20.0148\n",
            "Epoch [43/200], Train Loss: 24.5632, Val Loss: 18.1844\n",
            "Epoch [44/200], Train Loss: 30.5924, Val Loss: 20.8074\n",
            "Epoch [45/200], Train Loss: 24.5389, Val Loss: 19.1914\n",
            "Early stopping triggered after 45 epochs. Best epoch was 30 with val_loss 18.0434\n",
            "Validation MSE: 19.0291\n",
            "Saved new best model with validation MSE: 19.0291\n",
            "\n",
            "Trying hidden_size=32, num_layers=2\n",
            "Epoch [1/200], Train Loss: 570.7879, Val Loss: 543.3148\n",
            "Saved new best model with validation loss: 543.3148\n",
            "Epoch [2/200], Train Loss: 501.9683, Val Loss: 466.8700\n",
            "Saved new best model with validation loss: 466.8700\n",
            "Epoch [3/200], Train Loss: 425.4080, Val Loss: 382.9757\n",
            "Saved new best model with validation loss: 382.9757\n",
            "Epoch [4/200], Train Loss: 348.0036, Val Loss: 286.0948\n",
            "Saved new best model with validation loss: 286.0948\n",
            "Epoch [5/200], Train Loss: 248.1015, Val Loss: 180.9513\n",
            "Saved new best model with validation loss: 180.9513\n",
            "Epoch [6/200], Train Loss: 163.3619, Val Loss: 93.6574\n",
            "Saved new best model with validation loss: 93.6574\n",
            "Epoch [7/200], Train Loss: 95.7514, Val Loss: 61.0327\n",
            "Saved new best model with validation loss: 61.0327\n",
            "Epoch [8/200], Train Loss: 92.1941, Val Loss: 41.6655\n",
            "Saved new best model with validation loss: 41.6655\n",
            "Epoch [9/200], Train Loss: 67.6616, Val Loss: 31.8232\n",
            "Saved new best model with validation loss: 31.8232\n",
            "Epoch [10/200], Train Loss: 53.3451, Val Loss: 26.0415\n",
            "Saved new best model with validation loss: 26.0415\n",
            "Epoch [11/200], Train Loss: 55.7061, Val Loss: 23.7145\n",
            "Saved new best model with validation loss: 23.7145\n",
            "Epoch [12/200], Train Loss: 38.6400, Val Loss: 23.1858\n",
            "Saved new best model with validation loss: 23.1858\n",
            "Epoch [13/200], Train Loss: 39.2645, Val Loss: 21.1560\n",
            "Saved new best model with validation loss: 21.1560\n",
            "Epoch [14/200], Train Loss: 39.1906, Val Loss: 22.7702\n",
            "Epoch [15/200], Train Loss: 36.3491, Val Loss: 23.2435\n",
            "Epoch [16/200], Train Loss: 38.4348, Val Loss: 24.6814\n",
            "Epoch [17/200], Train Loss: 31.7255, Val Loss: 25.1493\n",
            "Epoch [18/200], Train Loss: 33.6278, Val Loss: 21.3850\n",
            "Epoch [19/200], Train Loss: 28.2138, Val Loss: 23.7871\n",
            "Epoch [20/200], Train Loss: 26.5465, Val Loss: 22.0117\n",
            "Epoch [21/200], Train Loss: 28.2297, Val Loss: 19.8058\n",
            "Saved new best model with validation loss: 19.8058\n",
            "Epoch [22/200], Train Loss: 30.8129, Val Loss: 21.7512\n",
            "Epoch [23/200], Train Loss: 27.8362, Val Loss: 20.5514\n",
            "Epoch [24/200], Train Loss: 28.0275, Val Loss: 20.1733\n",
            "Epoch [25/200], Train Loss: 28.9977, Val Loss: 22.0392\n",
            "Epoch [26/200], Train Loss: 26.9549, Val Loss: 23.9134\n",
            "Epoch [27/200], Train Loss: 27.1916, Val Loss: 22.5690\n",
            "Epoch [28/200], Train Loss: 29.2569, Val Loss: 21.6761\n",
            "Epoch [29/200], Train Loss: 25.7013, Val Loss: 20.6335\n",
            "Epoch [30/200], Train Loss: 34.9410, Val Loss: 20.8415\n",
            "Epoch [31/200], Train Loss: 28.4771, Val Loss: 19.8182\n",
            "Epoch [32/200], Train Loss: 28.9535, Val Loss: 19.5289\n",
            "Saved new best model with validation loss: 19.5289\n",
            "Epoch [33/200], Train Loss: 29.4789, Val Loss: 19.8716\n",
            "Epoch [34/200], Train Loss: 25.6781, Val Loss: 21.8126\n",
            "Epoch [35/200], Train Loss: 29.0706, Val Loss: 19.1099\n",
            "Saved new best model with validation loss: 19.1099\n",
            "Epoch [36/200], Train Loss: 28.7584, Val Loss: 20.2983\n",
            "Epoch [37/200], Train Loss: 31.4693, Val Loss: 19.6092\n",
            "Epoch [38/200], Train Loss: 39.1151, Val Loss: 21.5598\n",
            "Epoch [39/200], Train Loss: 27.0502, Val Loss: 21.2920\n",
            "Epoch [40/200], Train Loss: 28.0246, Val Loss: 25.5902\n",
            "Epoch [41/200], Train Loss: 27.0067, Val Loss: 20.7764\n",
            "Epoch [42/200], Train Loss: 24.3169, Val Loss: 22.1337\n",
            "Epoch [43/200], Train Loss: 25.9919, Val Loss: 21.0635\n",
            "Epoch [44/200], Train Loss: 23.8829, Val Loss: 18.9084\n",
            "Saved new best model with validation loss: 18.9084\n",
            "Epoch [45/200], Train Loss: 34.1278, Val Loss: 19.3653\n",
            "Epoch [46/200], Train Loss: 26.2069, Val Loss: 21.7544\n",
            "Epoch [47/200], Train Loss: 22.9882, Val Loss: 19.4709\n",
            "Epoch [48/200], Train Loss: 26.4267, Val Loss: 17.9305\n",
            "Saved new best model with validation loss: 17.9305\n",
            "Epoch [49/200], Train Loss: 27.2281, Val Loss: 20.2239\n",
            "Epoch [50/200], Train Loss: 20.1132, Val Loss: 20.7134\n",
            "Epoch [51/200], Train Loss: 40.3855, Val Loss: 22.1019\n",
            "Epoch [52/200], Train Loss: 21.9025, Val Loss: 19.4332\n",
            "Epoch [53/200], Train Loss: 25.2232, Val Loss: 19.5004\n",
            "Epoch [54/200], Train Loss: 24.9182, Val Loss: 19.3333\n",
            "Epoch [55/200], Train Loss: 19.1881, Val Loss: 17.5216\n",
            "Saved new best model with validation loss: 17.5216\n",
            "Epoch [56/200], Train Loss: 29.9625, Val Loss: 20.0330\n",
            "Epoch [57/200], Train Loss: 38.8472, Val Loss: 20.0217\n",
            "Epoch [58/200], Train Loss: 23.3737, Val Loss: 18.5593\n",
            "Epoch [59/200], Train Loss: 23.1660, Val Loss: 18.2694\n",
            "Epoch [60/200], Train Loss: 22.1735, Val Loss: 19.0724\n",
            "Epoch [61/200], Train Loss: 28.3940, Val Loss: 19.2588\n",
            "Epoch [62/200], Train Loss: 29.9902, Val Loss: 19.4265\n",
            "Epoch [63/200], Train Loss: 22.2655, Val Loss: 19.0028\n",
            "Epoch [64/200], Train Loss: 25.8284, Val Loss: 18.1569\n",
            "Epoch [65/200], Train Loss: 30.6639, Val Loss: 23.4581\n",
            "Epoch [66/200], Train Loss: 34.4358, Val Loss: 20.7472\n",
            "Epoch [67/200], Train Loss: 26.6187, Val Loss: 18.9418\n",
            "Epoch [68/200], Train Loss: 22.6683, Val Loss: 18.4532\n",
            "Epoch [69/200], Train Loss: 20.4823, Val Loss: 19.3489\n",
            "Epoch [70/200], Train Loss: 27.4846, Val Loss: 20.1833\n",
            "Early stopping triggered after 70 epochs. Best epoch was 55 with val_loss 17.5216\n",
            "Validation MSE: 20.3867\n",
            "\n",
            "Training final regression model with hidden_size=32, num_layers=1\n",
            "Initialized final model with best weights from hyperparameter search\n",
            "Epoch [1/300], Train Loss: 31.3239, Val Loss: 18.2879\n",
            "Saved new best model with validation loss: 18.2879\n",
            "Epoch [2/300], Train Loss: 43.1015, Val Loss: 25.4245\n",
            "Epoch [3/300], Train Loss: 33.0186, Val Loss: 17.5837\n",
            "Saved new best model with validation loss: 17.5837\n",
            "Epoch [4/300], Train Loss: 34.9736, Val Loss: 17.2770\n",
            "Saved new best model with validation loss: 17.2770\n",
            "Epoch [5/300], Train Loss: 32.9510, Val Loss: 18.1074\n",
            "Epoch [6/300], Train Loss: 22.5577, Val Loss: 18.6293\n",
            "Epoch [7/300], Train Loss: 29.3406, Val Loss: 18.5840\n",
            "Epoch [8/300], Train Loss: 23.1156, Val Loss: 17.4603\n",
            "Epoch [9/300], Train Loss: 49.2543, Val Loss: 20.0513\n",
            "Epoch [10/300], Train Loss: 25.1470, Val Loss: 18.7006\n",
            "Epoch [11/300], Train Loss: 26.9274, Val Loss: 18.6296\n",
            "Epoch [12/300], Train Loss: 35.0175, Val Loss: 17.9215\n",
            "Epoch [13/300], Train Loss: 24.1302, Val Loss: 18.0730\n",
            "Epoch [14/300], Train Loss: 30.9997, Val Loss: 17.8575\n",
            "Epoch [15/300], Train Loss: 26.0795, Val Loss: 18.5108\n",
            "Epoch [16/300], Train Loss: 49.2583, Val Loss: 18.2664\n",
            "Epoch [17/300], Train Loss: 37.0426, Val Loss: 17.4521\n",
            "Epoch [18/300], Train Loss: 27.1580, Val Loss: 17.4529\n",
            "Epoch [19/300], Train Loss: 29.3760, Val Loss: 18.3831\n",
            "Epoch [20/300], Train Loss: 29.3991, Val Loss: 18.2224\n",
            "Epoch [21/300], Train Loss: 23.7907, Val Loss: 18.1106\n",
            "Epoch [22/300], Train Loss: 29.8508, Val Loss: 17.7987\n",
            "Epoch [23/300], Train Loss: 29.5860, Val Loss: 17.8803\n",
            "Epoch [24/300], Train Loss: 28.5807, Val Loss: 18.0211\n",
            "Early stopping triggered after 24 epochs. Best epoch was 4 with val_loss 17.2770\n",
            "Final model saved to 'final_boston_model.pth'\n",
            "Test MSE: 14.4078\n",
            "Test RÂ²: 0.8035\n",
            "\n",
            "Attempting to analyze feature importance...\n",
            "Final Test MSE: 14.4078, RÂ²: 0.8035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and use a saved model\n",
        "def load_and_use_model(model_path):\n",
        "    \"\"\"\n",
        "    Load a saved model and return it ready for inference\n",
        "\n",
        "    Parameters:\n",
        "    model_path (str): Path to the saved model file\n",
        "\n",
        "    Returns:\n",
        "    model (ImprovedLiquidNeuralNetwork): Loaded model ready for inference\n",
        "    metadata (dict): Model metadata and information\n",
        "    \"\"\"\n",
        "    # Load the checkpoint\n",
        "    checkpoint = torch.load(model_path, weights_only=False)\n",
        "\n",
        "    # Extract hyperparameters\n",
        "    hyperparams = checkpoint['hyperparams']\n",
        "\n",
        "    # Create a new model with the same architecture\n",
        "    model = ImprovedLiquidNeuralNetwork(\n",
        "        input_size=hyperparams['input_size'],\n",
        "        hidden_size=hyperparams['hidden_size'],\n",
        "        output_size=hyperparams['output_size'],\n",
        "        num_layers=hyperparams['num_layers'],\n",
        "        task=hyperparams['task']\n",
        "    )\n",
        "\n",
        "    # Load the state dictionary\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Set to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Return the model and any metadata\n",
        "    metadata = {k: v for k, v in checkpoint.items() if k != 'model_state_dict'}\n",
        "\n",
        "    return model, metadata\n",
        "\n",
        "# Example of how to use the loaded model for inference\n",
        "def example_inference_cls(n_samples=5):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE: LOADING AND USING SAVED MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # Load the iris model\n",
        "        model, metadata = load_and_use_model('final_iris_model.pth')\n",
        "        print(\"Loaded Iris classification model\")\n",
        "        print(f\"Hyperparameters: {metadata['hyperparams']}\")\n",
        "\n",
        "        # Load a small sample of test data\n",
        "        from sklearn.datasets import load_iris\n",
        "        iris = load_iris()\n",
        "\n",
        "        # Choose only 5 samples - several ways to do this:\n",
        "        # Option 1: Take the first 5 samples\n",
        "        # Set a random seed for reproducibility\n",
        "        # np.random.seed(42)\n",
        "\n",
        "        # Choose 5 random samples\n",
        "        indices = np.random.choice(iris.data.shape[0], n_samples, replace=False)\n",
        "        X_sample = iris.data[indices]\n",
        "        y_sample = iris.target[indices]\n",
        "\n",
        "        # Get the class names\n",
        "        class_names = iris.target_names\n",
        "\n",
        "        # Preprocess (same as during training)\n",
        "        X_sample_tensor = torch.tensor(X_sample, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        # Make predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = model(X_sample_tensor)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # Initialize counts\n",
        "        correct = 0\n",
        "        total = len(y_sample)\n",
        "\n",
        "        print(\"\\nSample predictions:\")\n",
        "\n",
        "        for i in range(total):\n",
        "            sample = X_sample[i]\n",
        "            pred_class = metadata['target_names'][preds[i]]  # Predicted class name\n",
        "            actual_class = class_names[y_sample[i]]  # Actual class name\n",
        "\n",
        "            print(f\"Sample {i+1}: Features={sample.round(2)}, Predicted class={pred_class}, Actual class={actual_class}\")\n",
        "\n",
        "            # Compare prediction with actual class\n",
        "            if preds[i] == y_sample[i]:\n",
        "                correct += 1\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = correct / total * 100\n",
        "        print(f\"\\nClassification Accuracy: {accuracy:.2f}%\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error in example inference: {e}\")"
      ],
      "metadata": {
        "id": "Q704JOCPJP-E"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_inference_cls(n_samples=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmuBaUV9JW7G",
        "outputId": "4a3026b6-a7c9-4ac4-8301-3046ea434aec"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EXAMPLE: LOADING AND USING SAVED MODEL\n",
            "==================================================\n",
            "Loaded Iris classification model\n",
            "Hyperparameters: {'hidden_size': 32, 'num_layers': 2, 'task': 'cls', 'input_size': 4, 'output_size': 3}\n",
            "\n",
            "Sample predictions:\n",
            "Sample 1: Features=[5.  2.3 3.3 1. ], Predicted class=virginica, Actual class=versicolor\n",
            "Sample 2: Features=[5.2 3.5 1.5 0.2], Predicted class=setosa, Actual class=setosa\n",
            "Sample 3: Features=[6.7 3.3 5.7 2.1], Predicted class=virginica, Actual class=virginica\n",
            "Sample 4: Features=[6.2 2.8 4.8 1.8], Predicted class=virginica, Actual class=virginica\n",
            "Sample 5: Features=[6.9 3.1 4.9 1.5], Predicted class=virginica, Actual class=versicolor\n",
            "Sample 6: Features=[5.1 3.7 1.5 0.4], Predicted class=setosa, Actual class=setosa\n",
            "Sample 7: Features=[4.9 3.6 1.4 0.1], Predicted class=setosa, Actual class=setosa\n",
            "Sample 8: Features=[7.7 3.8 6.7 2.2], Predicted class=virginica, Actual class=virginica\n",
            "Sample 9: Features=[6.  2.2 5.  1.5], Predicted class=virginica, Actual class=virginica\n",
            "Sample 10: Features=[5.4 3.9 1.7 0.4], Predicted class=setosa, Actual class=setosa\n",
            "\n",
            "Classification Accuracy: 80.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference function\n",
        "def example_inference_reg(feature_names, n_samples=5):\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"EXAMPLE: LOADING AND USING SAVED REGRESSION MODEL\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    try:\n",
        "        # Load the California housing model\n",
        "        model, metadata = load_and_use_model('final_boston_model.pth')\n",
        "        print(\"Loaded California Housing regression model\")\n",
        "        print(f\"Hyperparameters: {metadata['hyperparams']}\")\n",
        "\n",
        "        # Create some sample data\n",
        "        from sklearn.datasets import fetch_california_housing\n",
        "        from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "        # Alternative if sklearn's boston dataset is deprecated\n",
        "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "        X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "        y = raw_df.values[1::2, 2]\n",
        "\n",
        "        # Choose only 5 samples - several ways to do this:\n",
        "        # Option 1: Take the first 5 samples\n",
        "        # Set a random seed for reproducibility\n",
        "        # np.random.seed(42)\n",
        "\n",
        "        # Choose 5 random samples\n",
        "        indices = np.random.choice(len(X), n_samples, replace=False)\n",
        "        X_sample = X[indices]\n",
        "        y_actual = y[indices]\n",
        "\n",
        "        # Preprocess (same as during training)\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(X_sample)  # Fit on all data since we don't have the exact scaler from training\n",
        "        X_sample_scaled = scaler.transform(X_sample)\n",
        "        X_sample_tensor = torch.tensor(X_sample_scaled, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension\n",
        "\n",
        "        # Make predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(X_sample_tensor)\n",
        "            predictions = predictions.numpy().flatten()\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\nSample predictions:\")\n",
        "        print(f\"{'Feature Values':<60} | {'Actual':<10} | {'Predicted':<10}\")\n",
        "        print(\"-\" * 85)\n",
        "\n",
        "        for i in range(len(X_sample)):\n",
        "            features_str = \", \".join([f\"{name}={val:.2f}\" for name, val in zip(feature_names, X_sample[i])])\n",
        "            print(f\"{features_str:<60} | {y_actual[i]:<10.2f} | {predictions[i]:<10.2f}\")\n",
        "\n",
        "        # Calculate error metrics for these samples\n",
        "        sample_mse = mean_squared_error(y_actual, predictions)\n",
        "        sample_r2 = r2_score(y_actual, predictions)\n",
        "        print(f\"\\nSample MSE: {sample_mse:.4f}, Sample RÂ²: {sample_r2:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in example inference: {e}\")"
      ],
      "metadata": {
        "id": "iTaHCABYKvMd"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',\n",
        "                        'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "example_inference_reg(feature_names, n_samples=20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n611eJoTLBNz",
        "outputId": "265db69d-8f85-4399-e444-0323f7c0e0c2"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "EXAMPLE: LOADING AND USING SAVED REGRESSION MODEL\n",
            "==================================================\n",
            "Loaded California Housing regression model\n",
            "Hyperparameters: {'hidden_size': 32, 'num_layers': 1, 'task': 'reg', 'input_size': 13, 'output_size': 1}\n",
            "\n",
            "Sample predictions:\n",
            "Feature Values                                               | Actual     | Predicted \n",
            "-------------------------------------------------------------------------------------\n",
            "CRIM=8.79, ZN=0.00, INDUS=18.10, CHAS=0.00, NOX=0.58, RM=5.57, AGE=70.60, DIS=2.06, RAD=24.00, TAX=666.00, PTRATIO=20.20, B=3.65, LSTAT=17.16 | 11.70      | 13.79     \n",
            "CRIM=0.09, ZN=0.00, INDUS=12.83, CHAS=0.00, NOX=0.44, RM=6.14, AGE=45.80, DIS=4.09, RAD=5.00, TAX=398.00, PTRATIO=18.70, B=386.96, LSTAT=10.27 | 20.80      | 21.28     \n",
            "CRIM=0.03, ZN=0.00, INDUS=5.19, CHAS=0.00, NOX=0.52, RM=5.87, AGE=46.30, DIS=5.23, RAD=5.00, TAX=224.00, PTRATIO=20.20, B=396.90, LSTAT=9.80 | 19.50      | 19.33     \n",
            "CRIM=9.82, ZN=0.00, INDUS=18.10, CHAS=0.00, NOX=0.67, RM=6.79, AGE=98.80, DIS=1.36, RAD=24.00, TAX=666.00, PTRATIO=20.20, B=396.90, LSTAT=21.24 | 13.30      | 11.95     \n",
            "CRIM=0.03, ZN=95.00, INDUS=1.47, CHAS=0.00, NOX=0.40, RM=6.97, AGE=15.30, DIS=7.65, RAD=3.00, TAX=402.00, PTRATIO=17.00, B=396.90, LSTAT=4.56 | 34.90      | 32.11     \n",
            "CRIM=0.11, ZN=30.00, INDUS=4.93, CHAS=0.00, NOX=0.43, RM=6.09, AGE=65.10, DIS=6.34, RAD=6.00, TAX=300.00, PTRATIO=16.60, B=394.62, LSTAT=12.40 | 20.10      | 20.30     \n",
            "CRIM=0.03, ZN=35.00, INDUS=6.06, CHAS=0.00, NOX=0.44, RM=6.03, AGE=23.30, DIS=6.64, RAD=1.00, TAX=304.00, PTRATIO=16.90, B=362.25, LSTAT=7.83 | 19.40      | 21.60     \n",
            "CRIM=0.04, ZN=80.00, INDUS=4.95, CHAS=0.00, NOX=0.41, RM=6.86, AGE=27.90, DIS=5.12, RAD=4.00, TAX=245.00, PTRATIO=19.20, B=396.90, LSTAT=3.33 | 28.50      | 35.02     \n",
            "CRIM=0.12, ZN=0.00, INDUS=2.89, CHAS=0.00, NOX=0.45, RM=6.62, AGE=57.80, DIS=3.50, RAD=2.00, TAX=276.00, PTRATIO=18.00, B=357.98, LSTAT=6.65 | 28.40      | 27.52     \n",
            "CRIM=0.49, ZN=0.00, INDUS=9.90, CHAS=0.00, NOX=0.54, RM=6.63, AGE=82.50, DIS=3.32, RAD=4.00, TAX=304.00, PTRATIO=18.40, B=396.90, LSTAT=4.54 | 22.80      | 29.66     \n",
            "CRIM=1.66, ZN=0.00, INDUS=19.58, CHAS=0.00, NOX=0.87, RM=6.12, AGE=97.30, DIS=1.62, RAD=5.00, TAX=403.00, PTRATIO=14.70, B=372.80, LSTAT=14.10 | 21.50      | 19.03     \n",
            "CRIM=14.42, ZN=0.00, INDUS=18.10, CHAS=0.00, NOX=0.74, RM=6.46, AGE=93.30, DIS=2.00, RAD=24.00, TAX=666.00, PTRATIO=20.20, B=27.49, LSTAT=18.05 | 9.60       | 11.00     \n",
            "CRIM=0.21, ZN=20.00, INDUS=3.33, CHAS=0.00, NOX=0.44, RM=6.81, AGE=32.20, DIS=4.10, RAD=5.00, TAX=216.00, PTRATIO=14.90, B=396.90, LSTAT=4.85 | 35.10      | 36.45     \n",
            "CRIM=0.14, ZN=12.50, INDUS=6.07, CHAS=0.00, NOX=0.41, RM=5.59, AGE=36.80, DIS=6.50, RAD=4.00, TAX=345.00, PTRATIO=18.90, B=396.90, LSTAT=13.09 | 17.40      | 17.82     \n",
            "CRIM=1.21, ZN=0.00, INDUS=19.58, CHAS=0.00, NOX=0.60, RM=5.88, AGE=94.60, DIS=2.43, RAD=5.00, TAX=403.00, PTRATIO=14.70, B=292.29, LSTAT=14.43 | 17.40      | 19.24     \n",
            "CRIM=1.41, ZN=0.00, INDUS=19.58, CHAS=1.00, NOX=0.87, RM=6.13, AGE=96.00, DIS=1.75, RAD=5.00, TAX=403.00, PTRATIO=14.70, B=321.02, LSTAT=15.12 | 17.00      | 20.22     \n",
            "CRIM=0.54, ZN=0.00, INDUS=6.20, CHAS=0.00, NOX=0.50, RM=5.98, AGE=68.10, DIS=3.67, RAD=8.00, TAX=307.00, PTRATIO=17.40, B=378.35, LSTAT=11.65 | 24.30      | 20.99     \n",
            "CRIM=0.76, ZN=20.00, INDUS=3.97, CHAS=0.00, NOX=0.65, RM=5.56, AGE=62.80, DIS=1.99, RAD=5.00, TAX=264.00, PTRATIO=13.00, B=392.40, LSTAT=10.45 | 22.80      | 22.80     \n",
            "CRIM=8.15, ZN=0.00, INDUS=18.10, CHAS=0.00, NOX=0.70, RM=5.39, AGE=98.90, DIS=1.73, RAD=24.00, TAX=666.00, PTRATIO=20.20, B=396.90, LSTAT=20.85 | 11.50      | 13.10     \n",
            "CRIM=0.06, ZN=0.00, INDUS=2.46, CHAS=0.00, NOX=0.49, RM=7.83, AGE=53.60, DIS=3.20, RAD=3.00, TAX=193.00, PTRATIO=17.80, B=392.63, LSTAT=4.45 | 50.00      | 47.51     \n",
            "\n",
            "Sample MSE: 7.6506, Sample RÂ²: 0.9121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced evaluation function\n",
        "def eval_liquid_regression(model_path, X_test, y_test, task='cls', feature_names=None, target_names=None):\n",
        "    # Load the California housing model\n",
        "    model, metadata = load_and_use_model(model_path)\n",
        "    print(f\"Hyperparameters: {metadata['hyperparams']}\")\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Create DataLoader for testing\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            outputs = model(batch_X)\n",
        "\n",
        "            if task == 'cls':\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                all_preds.extend(preds.numpy())\n",
        "                all_targets.extend(batch_y.numpy())\n",
        "            else:\n",
        "                all_preds.extend(outputs.squeeze().numpy())\n",
        "                all_targets.extend(batch_y.squeeze().numpy())\n",
        "\n",
        "    # Prepare metrics\n",
        "    if task == 'cls':\n",
        "        # Classification metrics\n",
        "        acc = accuracy_score(all_targets, all_preds)\n",
        "        cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "        print(f'Test Accuracy: {acc:.4f}')\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "\n",
        "        return acc\n",
        "    else:\n",
        "        # Regression metrics\n",
        "        mse = mean_squared_error(all_targets, all_preds)\n",
        "        r2 = r2_score(all_targets, all_preds)\n",
        "\n",
        "        print(f'Test MSE: {mse:.4f}')\n",
        "        print(f'Test RÂ²: {r2:.4f}')\n",
        "\n",
        "        return mse, r2"
      ],
      "metadata": {
        "id": "TDt2G6WvFJ-q"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Train and evaluate Linear Regression\n",
        "def linear_regression(X_train, X_test, y_train, y_test):\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    #print(f'Linear Regression MSE: {mse:.4f}')\n",
        "    #print(f'Linear Regression R2: {r2:.4f}')\n",
        "    return mse, r2\n",
        "\n",
        "# Train and evaluate Polynomial Regression\n",
        "def polynomial_regression(X_train, X_test, y_train, y_test, degree=2):\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_train_poly = poly.fit_transform(X_train)\n",
        "    X_test_poly = poly.transform(X_test)\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train_poly, y_train)\n",
        "    y_pred = model.predict(X_test_poly)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    #print(f'Polynomial Regression (degree={degree}) MSE: {mse:.4f}')\n",
        "    #print(f'Polynomial Regression (degree={degree}) R2: {r2:.4f}')\n",
        "    return mse, r2\n",
        "\n",
        "# Train and evaluate Support Vector Regression (SVR)\n",
        "def support_vector_regression(X_train, X_test, y_train, y_test):\n",
        "    model = SVR(kernel='rbf')\n",
        "    model.fit(X_train, y_train.ravel())  # Reshape y_train to 1D\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    #print(f'SVR MSE: {mse:.4f}')\n",
        "    #print(f'SVR R2: {r2:.4f}')\n",
        "    return mse, r2"
      ],
      "metadata": {
        "id": "c5m_uDr1y9qL"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edzrmDOxVR2z",
        "outputId": "2a03e256-36ce-42e2-8c15-ba73d056581f"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([102, 1, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test and evaluate Liquid Neural Network (Reg.)\n",
        "import torch\n",
        "\n",
        "model_reg = \"final_boston_model.pth\"\n",
        "\n",
        "if not isinstance(X_test, torch.Tensor):\n",
        "  X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "if not isinstance(y_test, torch.Tensor):\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "lnn_mse, lnn_r2 = eval_liquid_regression(model_reg, X_test, y_test, task='reg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAwzqH1qTfYp",
        "outputId": "774213be-a1fa-419d-d7ed-1a071f657356"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'hidden_size': 32, 'num_layers': 1, 'task': 'reg', 'input_size': 13, 'output_size': 1}\n",
            "Test MSE: 14.4078\n",
            "Test RÂ²: 0.8035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensor to Numpy\n",
        "import torch\n",
        "\n",
        "if isinstance(X_train, torch.Tensor):\n",
        "    print(\"X_train is a PyTorch tensor.\")\n",
        "    X_train=X_train.squeeze(1).cpu().detach().numpy()\n",
        "    y_train=y_train.squeeze(1).cpu().detach().numpy()\n",
        "\n",
        "    X_test=X_test.squeeze(1).cpu().detach().numpy()\n",
        "    y_test=y_test.squeeze(1).cpu().detach().numpy()\n",
        "else:\n",
        "    print(\"X_train is NOT a PyTorch tensor.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPN9PBFsTjUB",
        "outputId": "9aecb67e-996b-41f3-8713-a8f308913c12"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train is a PyTorch tensor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and evaluate Linear Regression\n",
        "lr_mse, lr_r2 = linear_regression(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Train and evaluate Polynomial Regression\n",
        "poly_mse, poly_r2 = polynomial_regression(X_train, X_test, y_train, y_test, degree=2)\n",
        "\n",
        "# Train and evaluate SVR\n",
        "svr_mse, svr_r2 = support_vector_regression(X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Compare performance\n",
        "print(\"\\nPerformance Comparison:\")\n",
        "print(f\"Liquid Neural Network (Reg.) MSE: {lnn_mse:.4f} || R2: {lnn_r2:.4f}\")\n",
        "print(f\"Linear Regression MSE: {lr_mse:.4f} || R2: {lr_r2:.4f}\")\n",
        "print(f\"Polynomial Regression MSE: {poly_mse:.4f} || R2: {poly_r2:.4f}\")\n",
        "print(f\"SVR MSE: {svr_mse:.4f} || R2: {svr_r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Crq-p8NtzZAy",
        "outputId": "147042bf-98a9-40c5-a4b1-fff79df9dc93"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Performance Comparison:\n",
            "Liquid Neural Network (Reg.) MSE: 14.4078 || R2: 0.8035\n",
            "Linear Regression MSE: 25.1021 || R2: 0.6577\n",
            "Polynomial Regression MSE: 13.6509 || R2: 0.8139\n",
            "SVR MSE: 29.0280 || R2: 0.6042\n"
          ]
        }
      ]
    }
  ]
}